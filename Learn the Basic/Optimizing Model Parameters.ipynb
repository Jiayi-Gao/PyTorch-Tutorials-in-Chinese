{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5985bd0a-4d3f-4d6b-9525-60ced63fef86",
   "metadata": {},
   "source": [
    "# OPTIMIZING MODEL PARAMETERS\n",
    "\n",
    "现在我们有了模型和数据，是时候通过优化参数对我们的模型进行训练、验证和测试了。训练模型是一个迭代的过程；在每一次迭代中模型会预测一个输出，并对它的输出计算一个误差（损失），收集这些相关参数的误差的导数（如上一节所示），并使用梯度下降**优化**这些参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa362b38-109b-411c-9be0-b250045d53c5",
   "metadata": {},
   "source": [
    "## Prerequisite Code\n",
    "\n",
    "我们从之前的Datasets & DataLoaders和Build Model章节中加载代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d888a15-a176-4d0d-990b-6e11b1b625d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sda/home/gaojiayi/anaconda3/envs/jpth/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90076661-70cb-40bb-9794-2f8b493d4ad2",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "超参数是让你控制模型优化过程的可调参数。不同的超参数值可能会影响模型的训练和收敛率（[阅读更多](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)有关超参数调整的内容）。\n",
    "\n",
    "我们定义了如下的超参数用于训练：\n",
    "- **Number of Epoch（世代数）** 迭代数据集的次数\n",
    "- **Batch Size（批量大小）** 参数更新前通过网络传播的数据样本数\n",
    "- **Learning Rate（学习率）**在每个批次/阶段更新模型参数的程度。较小的值产生缓慢的学习速度，而较大的值可能导致训练期间的不可预测的行为。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7169d39-e8b1-46d1-aa9d-effa34a53c76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5e2f3-607c-4205-a5e3-59c798eb17ce",
   "metadata": {},
   "source": [
    "## Optimization Loop\n",
    "\n",
    "当我们设置好我们的超参数，我们就可以通过优化循环训练和优化我们的模型。每个优化循环的迭代乘以一个**世代(epoch)**。\n",
    "\n",
    "每个世代由两个主要的部分组成：\n",
    "- **训练循环** 迭代训练集并且尝试通过优化参数收敛模型。\n",
    "- **验证/测试循环** 迭代测试集并查看模型性能是否有所提升。\n",
    "\n",
    "\n",
    "让我们简单地熟悉一下训练循环中使用的一些概念。在这里看看优化循环的[完整实现](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#full-impl-label)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf7583f-a379-41a8-a0ee-984cc80d30ec",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "当遇到一些训练数据时，我们未经训练的网络给出的答案很可能是错误的。**损失函数(Loss funcation)** 衡量的是模型输出的结果与目标值的不相似程度，它是我们在训练期间想要最小化的损失函数。为了计算损失，我们使用给定数据样本的输入进行预测，并与真实数据标签值进行比较。\n",
    "\n",
    "常用的损失函数包括回归任务的[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)(均方误差)，分类任务的[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)(负对数似然)。[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)结合了`nn.LogSoftmax`和`nn.NLLLoss`。\n",
    "\n",
    "我们将模型的输出对数传递给`nn.CrossEntropyLoss`，它将对对数进行归一化处理并计算预测误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540c6774-c3d5-4492-baac-f45490fa1d50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初始化损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b8b015-b57f-48ac-9eeb-f7f83c43158f",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "优化是在每一步训练中调节模型参数以减少模型误差的过程。**优化算法** 定义了这个过程如何被执行（在这个例子中我们使用了随机梯度下降）。所有的优化逻辑都被封装在优化器对象中。在这里，我们使用SGD优化器；此外，PyTorch中还有许多[不同的优化器](https://pytorch.org/docs/stable/optim.html)，如ADAM和RMSProp，它们对不同类型的模型和数据有更好的效果。\n",
    "\n",
    "我们通过注册需要训练的模型参数来初始化优化器，并传入学习率超参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4b138d6-7b93-47e5-997d-3feeca580907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff17dc9-902d-41c3-a24d-1a0cd51e638a",
   "metadata": {},
   "source": [
    "在训练循环中，优化会发生在三个步骤中：\n",
    "- 调用`optimizer.zero_grad()`来重新设置模型参数的梯度。梯度默认为累加；为了防止重复计算，我们在每次迭代时都明确地将其归零。\n",
    "- 通过调用 `loss.backward()` 对预测损失进行反向传播。PyTorch将损失的梯度与每个参数结合起来。\n",
    "- 一旦我们有了梯度，我们就调用 `optimizer.step()`，通过后向传递中收集的梯度来调整参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f1b95e-bc55-4116-bddf-c6c747847435",
   "metadata": {},
   "source": [
    "## Full Implementation\n",
    "\n",
    "我们定义了 `train_loop` 和 `test_loop` ，`train_loop`负责循环我们的优化代码，`test_loop` 负责根据测试数据评估模型的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d2d77f-6b38-4dd7-9ba6-b9ed06bb69a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 计算预测和损失\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # 后向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "            \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in  dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy:{(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1405321d-57e0-443d-b0d3-e63b26bb8e25",
   "metadata": {},
   "source": [
    "我们初始化损失函数和优化器，并将其传递给`train_loop`和`test_loop`。增加epochs的数量来跟踪模型的改进性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dfdad47-15f7-44e9-b221-493d72bf4157",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-----------------------------\n",
      "loss: 2.302937 [   64/60000]\n",
      "loss: 2.295795 [ 6464/60000]\n",
      "loss: 2.272993 [12864/60000]\n",
      "loss: 2.268931 [19264/60000]\n",
      "loss: 2.258493 [25664/60000]\n",
      "loss: 2.215361 [32064/60000]\n",
      "loss: 2.237937 [38464/60000]\n",
      "loss: 2.200621 [44864/60000]\n",
      "loss: 2.193541 [51264/60000]\n",
      "loss: 2.172049 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:35.8%, Avg loss: 2.156936 \n",
      "\n",
      "Epoch 2\n",
      "-----------------------------\n",
      "loss: 2.167597 [   64/60000]\n",
      "loss: 2.164003 [ 6464/60000]\n",
      "loss: 2.097303 [12864/60000]\n",
      "loss: 2.121943 [19264/60000]\n",
      "loss: 2.075570 [25664/60000]\n",
      "loss: 2.008105 [32064/60000]\n",
      "loss: 2.052706 [38464/60000]\n",
      "loss: 1.969182 [44864/60000]\n",
      "loss: 1.969421 [51264/60000]\n",
      "loss: 1.916238 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:59.1%, Avg loss: 1.896470 \n",
      "\n",
      "Epoch 3\n",
      "-----------------------------\n",
      "loss: 1.927772 [   64/60000]\n",
      "loss: 1.904061 [ 6464/60000]\n",
      "loss: 1.771504 [12864/60000]\n",
      "loss: 1.826562 [19264/60000]\n",
      "loss: 1.714042 [25664/60000]\n",
      "loss: 1.662547 [32064/60000]\n",
      "loss: 1.702084 [38464/60000]\n",
      "loss: 1.591118 [44864/60000]\n",
      "loss: 1.611062 [51264/60000]\n",
      "loss: 1.523414 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:61.1%, Avg loss: 1.520753 \n",
      "\n",
      "Epoch 4\n",
      "-----------------------------\n",
      "loss: 1.589820 [   64/60000]\n",
      "loss: 1.557414 [ 6464/60000]\n",
      "loss: 1.389673 [12864/60000]\n",
      "loss: 1.475070 [19264/60000]\n",
      "loss: 1.348577 [25664/60000]\n",
      "loss: 1.345520 [32064/60000]\n",
      "loss: 1.369854 [38464/60000]\n",
      "loss: 1.289521 [44864/60000]\n",
      "loss: 1.321150 [51264/60000]\n",
      "loss: 1.231017 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:62.9%, Avg loss: 1.248507 \n",
      "\n",
      "Epoch 5\n",
      "-----------------------------\n",
      "loss: 1.329382 [   64/60000]\n",
      "loss: 1.314144 [ 6464/60000]\n",
      "loss: 1.135634 [12864/60000]\n",
      "loss: 1.249584 [19264/60000]\n",
      "loss: 1.118476 [25664/60000]\n",
      "loss: 1.144912 [32064/60000]\n",
      "loss: 1.170493 [38464/60000]\n",
      "loss: 1.108743 [44864/60000]\n",
      "loss: 1.146644 [51264/60000]\n",
      "loss: 1.068688 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:64.3%, Avg loss: 1.084930 \n",
      "\n",
      "Epoch 6\n",
      "-----------------------------\n",
      "loss: 1.159360 [   64/60000]\n",
      "loss: 1.164275 [ 6464/60000]\n",
      "loss: 0.972235 [12864/60000]\n",
      "loss: 1.113764 [19264/60000]\n",
      "loss: 0.978511 [25664/60000]\n",
      "loss: 1.012667 [32064/60000]\n",
      "loss: 1.051962 [38464/60000]\n",
      "loss: 0.996763 [44864/60000]\n",
      "loss: 1.035620 [51264/60000]\n",
      "loss: 0.970570 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:65.6%, Avg loss: 0.980911 \n",
      "\n",
      "Epoch 7\n",
      "-----------------------------\n",
      "loss: 1.042292 [   64/60000]\n",
      "loss: 1.067560 [ 6464/60000]\n",
      "loss: 0.861005 [12864/60000]\n",
      "loss: 1.024543 [19264/60000]\n",
      "loss: 0.890167 [25664/60000]\n",
      "loss: 0.919161 [32064/60000]\n",
      "loss: 0.975594 [38464/60000]\n",
      "loss: 0.924430 [44864/60000]\n",
      "loss: 0.959018 [51264/60000]\n",
      "loss: 0.904858 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:66.9%, Avg loss: 0.909705 \n",
      "\n",
      "Epoch 8\n",
      "-----------------------------\n",
      "loss: 0.955509 [   64/60000]\n",
      "loss: 0.999538 [ 6464/60000]\n",
      "loss: 0.781058 [12864/60000]\n",
      "loss: 0.960619 [19264/60000]\n",
      "loss: 0.830184 [25664/60000]\n",
      "loss: 0.849515 [32064/60000]\n",
      "loss: 0.921655 [38464/60000]\n",
      "loss: 0.875938 [44864/60000]\n",
      "loss: 0.903234 [51264/60000]\n",
      "loss: 0.856677 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:68.0%, Avg loss: 0.857845 \n",
      "\n",
      "Epoch 9\n",
      "-----------------------------\n",
      "loss: 0.888148 [   64/60000]\n",
      "loss: 0.947437 [ 6464/60000]\n",
      "loss: 0.720859 [12864/60000]\n",
      "loss: 0.911996 [19264/60000]\n",
      "loss: 0.787330 [25664/60000]\n",
      "loss: 0.796588 [32064/60000]\n",
      "loss: 0.880394 [38464/60000]\n",
      "loss: 0.842244 [44864/60000]\n",
      "loss: 0.861070 [51264/60000]\n",
      "loss: 0.819515 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:69.5%, Avg loss: 0.818227 \n",
      "\n",
      "Epoch 10\n",
      "-----------------------------\n",
      "loss: 0.833922 [   64/60000]\n",
      "loss: 0.904735 [ 6464/60000]\n",
      "loss: 0.674137 [12864/60000]\n",
      "loss: 0.873786 [19264/60000]\n",
      "loss: 0.754803 [25664/60000]\n",
      "loss: 0.755960 [32064/60000]\n",
      "loss: 0.846978 [38464/60000]\n",
      "loss: 0.817616 [44864/60000]\n",
      "loss: 0.828004 [51264/60000]\n",
      "loss: 0.789368 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:70.6%, Avg loss: 0.786628 \n",
      "\n",
      "Epoch 11\n",
      "-----------------------------\n",
      "loss: 0.788842 [   64/60000]\n",
      "loss: 0.868336 [ 6464/60000]\n",
      "loss: 0.636534 [12864/60000]\n",
      "loss: 0.842963 [19264/60000]\n",
      "loss: 0.728913 [25664/60000]\n",
      "loss: 0.724064 [32064/60000]\n",
      "loss: 0.818593 [38464/60000]\n",
      "loss: 0.798253 [44864/60000]\n",
      "loss: 0.801116 [51264/60000]\n",
      "loss: 0.763950 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:72.0%, Avg loss: 0.760347 \n",
      "\n",
      "Epoch 12\n",
      "-----------------------------\n",
      "loss: 0.750375 [   64/60000]\n",
      "loss: 0.836365 [ 6464/60000]\n",
      "loss: 0.605117 [12864/60000]\n",
      "loss: 0.817474 [19264/60000]\n",
      "loss: 0.707266 [25664/60000]\n",
      "loss: 0.698427 [32064/60000]\n",
      "loss: 0.793441 [38464/60000]\n",
      "loss: 0.781942 [44864/60000]\n",
      "loss: 0.778352 [51264/60000]\n",
      "loss: 0.741788 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:73.2%, Avg loss: 0.737657 \n",
      "\n",
      "Epoch 13\n",
      "-----------------------------\n",
      "loss: 0.716861 [   64/60000]\n",
      "loss: 0.807559 [ 6464/60000]\n",
      "loss: 0.578164 [12864/60000]\n",
      "loss: 0.795803 [19264/60000]\n",
      "loss: 0.688650 [25664/60000]\n",
      "loss: 0.677306 [32064/60000]\n",
      "loss: 0.770612 [38464/60000]\n",
      "loss: 0.767352 [44864/60000]\n",
      "loss: 0.758603 [51264/60000]\n",
      "loss: 0.722071 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:74.3%, Avg loss: 0.717488 \n",
      "\n",
      "Epoch 14\n",
      "-----------------------------\n",
      "loss: 0.686985 [   64/60000]\n",
      "loss: 0.781386 [ 6464/60000]\n",
      "loss: 0.554448 [12864/60000]\n",
      "loss: 0.776984 [19264/60000]\n",
      "loss: 0.672662 [25664/60000]\n",
      "loss: 0.659462 [32064/60000]\n",
      "loss: 0.749468 [38464/60000]\n",
      "loss: 0.754103 [44864/60000]\n",
      "loss: 0.741179 [51264/60000]\n",
      "loss: 0.704256 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:75.0%, Avg loss: 0.699285 \n",
      "\n",
      "Epoch 15\n",
      "-----------------------------\n",
      "loss: 0.660287 [   64/60000]\n",
      "loss: 0.757494 [ 6464/60000]\n",
      "loss: 0.533407 [12864/60000]\n",
      "loss: 0.760281 [19264/60000]\n",
      "loss: 0.658579 [25664/60000]\n",
      "loss: 0.644113 [32064/60000]\n",
      "loss: 0.729895 [38464/60000]\n",
      "loss: 0.741999 [44864/60000]\n",
      "loss: 0.725751 [51264/60000]\n",
      "loss: 0.687968 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:75.8%, Avg loss: 0.682680 \n",
      "\n",
      "Epoch 16\n",
      "-----------------------------\n",
      "loss: 0.636219 [   64/60000]\n",
      "loss: 0.735537 [ 6464/60000]\n",
      "loss: 0.514556 [12864/60000]\n",
      "loss: 0.745189 [19264/60000]\n",
      "loss: 0.646147 [25664/60000]\n",
      "loss: 0.630782 [32064/60000]\n",
      "loss: 0.711580 [38464/60000]\n",
      "loss: 0.730876 [44864/60000]\n",
      "loss: 0.712052 [51264/60000]\n",
      "loss: 0.673041 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:76.5%, Avg loss: 0.667431 \n",
      "\n",
      "Epoch 17\n",
      "-----------------------------\n",
      "loss: 0.614376 [   64/60000]\n",
      "loss: 0.715395 [ 6464/60000]\n",
      "loss: 0.497552 [12864/60000]\n",
      "loss: 0.731395 [19264/60000]\n",
      "loss: 0.635039 [25664/60000]\n",
      "loss: 0.619124 [32064/60000]\n",
      "loss: 0.694597 [38464/60000]\n",
      "loss: 0.720759 [44864/60000]\n",
      "loss: 0.699871 [51264/60000]\n",
      "loss: 0.659250 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:77.1%, Avg loss: 0.653397 \n",
      "\n",
      "Epoch 18\n",
      "-----------------------------\n",
      "loss: 0.594564 [   64/60000]\n",
      "loss: 0.696933 [ 6464/60000]\n",
      "loss: 0.482134 [12864/60000]\n",
      "loss: 0.718623 [19264/60000]\n",
      "loss: 0.625210 [25664/60000]\n",
      "loss: 0.608762 [32064/60000]\n",
      "loss: 0.678828 [38464/60000]\n",
      "loss: 0.711614 [44864/60000]\n",
      "loss: 0.689190 [51264/60000]\n",
      "loss: 0.646377 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:77.7%, Avg loss: 0.640483 \n",
      "\n",
      "Epoch 19\n",
      "-----------------------------\n",
      "loss: 0.576576 [   64/60000]\n",
      "loss: 0.679925 [ 6464/60000]\n",
      "loss: 0.468182 [12864/60000]\n",
      "loss: 0.706759 [19264/60000]\n",
      "loss: 0.616386 [25664/60000]\n",
      "loss: 0.599511 [32064/60000]\n",
      "loss: 0.664156 [38464/60000]\n",
      "loss: 0.703530 [44864/60000]\n",
      "loss: 0.679773 [51264/60000]\n",
      "loss: 0.634467 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:78.1%, Avg loss: 0.628595 \n",
      "\n",
      "Epoch 20\n",
      "-----------------------------\n",
      "loss: 0.560319 [   64/60000]\n",
      "loss: 0.664229 [ 6464/60000]\n",
      "loss: 0.455474 [12864/60000]\n",
      "loss: 0.695635 [19264/60000]\n",
      "loss: 0.608424 [25664/60000]\n",
      "loss: 0.591147 [32064/60000]\n",
      "loss: 0.650720 [38464/60000]\n",
      "loss: 0.696588 [44864/60000]\n",
      "loss: 0.671448 [51264/60000]\n",
      "loss: 0.623334 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy:78.6%, Avg loss: 0.617666 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-----------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d2a558-26e7-4003-b6cc-640b8af1b155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpth",
   "language": "python",
   "name": "jpth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
