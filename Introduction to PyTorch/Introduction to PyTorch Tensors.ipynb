{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3d71b3-04e6-4d7d-a3c5-a61b5b4fa748",
   "metadata": {},
   "source": [
    "# INTRODUCTION TO PYTORCH TENSORS\n",
    "\n",
    "<img src=\"https://pytorch.org/tutorials/_static/images/pytorch-colab.svg\" width = \"25\"/>  [Colab](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/63a0f0fc7b3ffb15d3a5ac8db3d521ee/tensors_deeper_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8adb9-a9b7-411b-b23c-3593998aef62",
   "metadata": {},
   "source": [
    "张量是PyTorch的核心数据抽象。本节对 `torch.Tensor` 类进行了深入介绍。\n",
    "\n",
    "首先，让我们导入PyTorch模块。我们还需导入Python的数学模块，以便一些例子的学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c010765a-40c1-4e58-9e8c-fdbc480d3b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a187b117-f64f-4ba3-9459-5e0c0f3da3c8",
   "metadata": {},
   "source": [
    "## Creating Tensors\n",
    "\n",
    "调用 `torch.empty()` 是创建张量最简单的方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3901e12a-8dd7-4590-9c7d-22a1e52756e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[ 4.4491e-09,  4.5612e-41,  4.4491e-09,  4.5612e-41],\n",
      "        [-7.0057e-06,  3.2926e+06,  3.6110e+17,  4.5605e-41],\n",
      "        [ 2.3109e+17,  4.5605e-41,  3.4535e+23, -4.0666e+12]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3, 4)\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faffe379-7264-4ec9-a579-9d0b7fc1fd3c",
   "metadata": {},
   "source": [
    "让我们将刚刚所做的分解一下：\n",
    "\n",
    "- 我们使用 `torch` 模块的众多方法之一创建了一个张量。\n",
    "- 创建的是一个二维张量，它有三行四列。\n",
    "- 返回的对象的类型是torch.Tensor，它是torch.FloatTensor的别名；默认情况下，PyTorch张量是32位浮点数类型的。(下面有更多关于数据类型的内容)。\n",
    "- 在打印你的张量时，你可能会看到一些随机的值。调用 `torch.empty()` 为张量分配了内存，但并没有用任何值来初始化它--所以你所看到的是分配时内存中的存储的东西。\n",
    "\n",
    "关于张量及其维数的简要说明，以及术语：\n",
    "\n",
    "- 你会看到一个一维的张量被称为向量。\n",
    "\n",
    "- 同样，一个二维的张量经常被称为矩阵。\n",
    "\n",
    "- 任何有两个维度以上的东西一般都被称为张量。\n",
    "\n",
    "更多的时候，你会想用一些值来初始化你的张量。常见的情况是所有的零、所有的一或随机值，而 `torch` 模块为所有这些提供出厂方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfa6d6a6-5910-4369-b808-f0a6649197e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros(2, 3)\n",
    "print(zeros)\n",
    "\n",
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random = torch.rand(2, 3)\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b5290d-acc4-485c-bf9e-96503a047924",
   "metadata": {},
   "source": [
    "## Random Tensors and Seeding\n",
    "\n",
    "说到随机张量，你是否注意到紧接着对 `torch.manual_seed()` 的调用？用随机值初始化张量，比如模型的学习权重，是很常见的，但是有些时候--特别是在研究环境中--你会希望对你的结果的可重复性有一些保证。手动设置你的随机数发生器的种子就是这样做的。让我们详细地看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbaa7fc7-9364-4dce-b10d-ea39f68806f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d19088-5e75-4233-a6ac-e04477365f11",
   "metadata": {},
   "source": [
    "你在上面应该看到的是，`random1` 和 `random3` 带有相同的值，`random2` 和 `random4` 也是如此。手动设置RNGa的种子会重置它，所以在大多数情况下，取决于随机数相同的计算应该提供相同的结果。\n",
    "\n",
    "更对详细信息, 请见[PyTorch documentation on reproducibility](https://pytorch.org/docs/stable/notes/randomness.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69302bfd-2e2f-4618-836a-869c5cf009de",
   "metadata": {},
   "source": [
    "## Tensor Shapes\n",
    "\n",
    "通常，当你对两个或多个张量进行操作时，它们需要具有相同的形状--即具有相同的维数和每个维数中相同的单元数。为此，我们有 `torch.*_like()` 方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b58fa3c2-d2e3-444e-9c81-45b09364a986",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 8.5352e-01, 3.0792e-41]],\n",
      "\n",
      "        [[1.5187e-01, 3.0792e-41, 4.8903e+16],\n",
      "         [4.5605e-41, 1.4013e-45, 0.0000e+00]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[8.6412e-01, 3.0792e-41, 0.0000e+00],\n",
      "         [1.5910e-05, 0.0000e+00, 1.5269e+00]],\n",
      "\n",
      "        [[0.0000e+00, 1.5910e-05, 0.0000e+00],\n",
      "         [2.2972e+02, 0.0000e+00, 1.7140e-05]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.6128, 0.1519, 0.0453],\n",
      "         [0.5035, 0.9978, 0.3884]],\n",
      "\n",
      "        [[0.6929, 0.1703, 0.1384],\n",
      "         [0.4759, 0.7481, 0.0361]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2, 2, 3)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "empty_like_x = torch.empty_like(x)\n",
    "print(empty_like_x.shape)\n",
    "print(empty_like_x)\n",
    "\n",
    "zeros_like_x = torch.zeros_like(x)\n",
    "print(zeros_like_x.shape)\n",
    "print(zeros_like_x)\n",
    "\n",
    "ones_like_x = torch.ones_like(x)\n",
    "print(ones_like_x.shape)\n",
    "print(ones_like_x)\n",
    "\n",
    "rand_like_x = torch.rand_like(x)\n",
    "print(rand_like_x.shape)\n",
    "print(rand_like_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe236d9-52db-4314-b135-447d9a0fa3d0",
   "metadata": {},
   "source": [
    "上面代码单元中在张量上使用 `.shape` 属性。这个属性包含一个张量的每个维度的范围列表--在我们的例子中，`x` 是一个三维张量，形状为2 x 2 x 3。\n",
    "\n",
    "在这下面，我们调用 `.empty_like()` 、`.zeros_like()` 、`.one_like()` 和 `.rand_like()` 方法。使用 `.shape` 属性，我们可以验证这些方法中的每一个都返回一个维度和范围相同的张量。\n",
    "\n",
    "最后一种创建一个张量的方法是直接从PyTorch集合中指定其数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43000acd-aabf-487d-b312-2fb135fd7a19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1416, 2.7183],\n",
      "        [1.6180, 0.0073]])\n",
      "tensor([ 2,  3,  5,  7, 11, 13, 17, 19])\n",
      "tensor([[2, 4, 6],\n",
      "        [3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])\n",
    "print(some_constants)\n",
    "\n",
    "some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))\n",
    "print(some_integers)\n",
    "\n",
    "more_integers = torch.tensor(((2, 4, 6), [3, 6, 9]))\n",
    "print(more_integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb891d-3c4b-4414-940f-15f6b741d70e",
   "metadata": {},
   "source": [
    "如果你已经有了Python元组或列表中的数据，创建张量最直接的方法是使用 `torch.tensor()` 。如上所示，嵌套集合将产生一个多维张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b946f8d4-8f0b-4f2a-9bb2-c56361f1d478",
   "metadata": {},
   "source": [
    "> **小贴士**\n",
    ">\n",
    "> `torch.tensor()` 创建了数据的一份拷贝。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd8b6ad-ad1c-4cec-9fda-ffdf5447ea82",
   "metadata": {},
   "source": [
    "## Tensor Data Types\n",
    "\n",
    "设置张量的数据类型有几种方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5bc9379-f750-4d21-9f01-f24f6aad81f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[ 0.9956,  1.4148,  5.8364],\n",
      "        [11.2406, 11.2083, 11.6692]], dtype=torch.float64)\n",
      "tensor([[ 0,  1,  5],\n",
      "        [11, 11, 11]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(a)\n",
    "\n",
    "b = torch.rand((2, 3), dtype=torch.float64) * 20\n",
    "print(b)\n",
    "\n",
    "c = b.to(torch.int32)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21234179-3870-438a-a575-e74b01569d15",
   "metadata": {},
   "source": [
    "设置张量的底层数据类型的最简单方法是在创建时使用一个可选的参数。在上面单元格的第一行，我们为张量 `a` 设置 `dtype=torch.int16`。当我们打印 `a` 时，我们可以看到它的填充值为 `1`，而不是 `1.` --这是Python的一个巧妙的机制，提示我们这是一个整数类型，而不是浮点类型。\n",
    "\n",
    "关于打印 `a`，需要注意的另一件事是，与我们将 `dtype` 作为默认值（32位浮点）不同，打印张量时会指明其 `dtype`。\n",
    "\n",
    "你可能还发现，我们从一系列的整数参数指定张量的形状，到将这些参数分组为一个元组。这并不是必须的--PyTorch会将一系列初始的、未标记的整数参数作为张量的形状--但在添加可选的参数时，它可以使你的意图更容易阅读。\n",
    "\n",
    "设置数据类型的另一种方法是使用 `.to()` 方法。在上面的单元格中，我们创建一个随机的浮点张量 `b`。之后，我们用 `.to()` 方法将b转换为32位整数，从而创建 `c` 。注意，`c` 包含所有与 `b` 相同的值，但被截断为整数。\n",
    "\n",
    "可用的数据类型包括：\n",
    "\n",
    "- `torch.bool`\n",
    "- `torch.int8`\n",
    "- `torch.uint8`\n",
    "- `torch.int16`\n",
    "- `torch.int32`\n",
    "- `torch.int64`\n",
    "- `torch.half`\n",
    "- `torch.float`\n",
    "- `torch.double`\n",
    "- `torch.bfloat`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb7bf7-5a8d-4d3e-9f7d-66a7f99b17dd",
   "metadata": {},
   "source": [
    "## Math & Logic with PyTorch Tensors\n",
    "\n",
    "现在，你知道了一些创建张量的方法...那么你可以对他做什么呢？\n",
    "\n",
    "让我们先看看基本的算术，以及张量如何与简单的标量进行运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6998c55-f076-443f-aaf7-0b071192c6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.zeros(2, 2) + 1\n",
    "twos = torch.ones(2, 2) * 2\n",
    "threes = (torch.ones(2, 2) * 7 - 1) / 2\n",
    "fours = twos ** 2\n",
    "sqrt2s = twos ** 0.5\n",
    "\n",
    "print(ones)\n",
    "print(twos)\n",
    "print(threes)\n",
    "print(fours)\n",
    "print(sqrt2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85dcd5a-00df-46ec-a720-46221d36c5e2",
   "metadata": {},
   "source": [
    "正如你在上面看到的，张量和标量之间的算术运算，如加法、减法、乘法、除法和指数化都分布在张量的每个元素上。因为这种操作的输出将是一个张量，你可以用通常的运算符优先规则将它们链接起来，就像我们创建 `threes` 的那一行。\n",
    "\n",
    "两个张量之间的类似操作也是如此："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3ffae13-117b-4518-9804-5725b99c6fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 16.]])\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]])\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "powers2 = twos ** torch.tensor([[1, 2], [3, 4]])\n",
    "print(powers2)\n",
    "\n",
    "fives = ones + fours\n",
    "print(fives)\n",
    "\n",
    "dozens = threes * fours\n",
    "print(dozens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb13d746-724b-413a-83de-0030135f627d",
   "metadata": {},
   "source": [
    "在一般情况下，你不能以这种方式对不同形状的张量进行操作，即使是在像上面的单元格那样的情况下，张量的元素数量相同，也不能对不同形状的张量进行操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69eb6af-0b45-46f9-bdc8-4f5c27c1df8f",
   "metadata": {},
   "source": [
    "## In Brief: Tensor Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed37d37c-ade6-4aaf-8a5d-ec70e5c85daa",
   "metadata": {},
   "source": [
    "> **小贴士**\n",
    ">\n",
    "> 如果你熟悉NumPy ndarrays中的广播机制，你会发现这里也适用同样的规则。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb815ba4-541e-47e9-a38a-833e63d72774",
   "metadata": {
    "tags": []
   },
   "source": [
    "与相同形状规则所不同的是张量的广播。请看下面的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a01e5c9-cc47-419d-96d0-23161b0e11e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6146, 0.5999, 0.5013, 0.9397],\n",
      "        [0.8656, 0.5207, 0.6865, 0.3614]])\n",
      "tensor([[1.2291, 1.1998, 1.0026, 1.8793],\n",
      "        [1.7312, 1.0413, 1.3730, 0.7228]])\n"
     ]
    }
   ],
   "source": [
    "rand = torch.rand(2, 4)\n",
    "doubled = rand * (torch.ones(1, 4) * 2)\n",
    "\n",
    "print(rand)\n",
    "print(doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b6f97-b1e4-49ff-91a3-2a6af3dff3c8",
   "metadata": {},
   "source": [
    "这里有什么技巧？我们是如何让2x4的张量和1x4的张量相乘的？\n",
    "\n",
    "广播是在形状相似的张量之间进行操作的一种方式。在上面的例子中，1行4列的张量与2行4列的张量的两行都进行了相乘。\n",
    "\n",
    "这是深度学习中的一个重要操作。常见的情况是将学习权重的张量与一批输入张量相乘，对该批中的每个实例分别应用该操作，并返回一个形状相同的张量--就像我们上面的 (2，4) * (1，4) 的例子一样，返回一个形状为（2，4）的张量。\n",
    "\n",
    "广播的规则是：\n",
    "\n",
    "- 每个张量至少有一维，不能是空张量。\n",
    "- 从尾到头比较两个张量的维度大小:\n",
    "    - 每个维度必须相等，或\n",
    "    - 一个维度必须为1，或\n",
    "    - 维度不存在于其中一个张量\n",
    "    \n",
    "当然，形状相同的张量一定 \"可广播\"，正如你前面所看到的。\n",
    "\n",
    "下面是一些遵守上述规则并允许广播的情况的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75c8c919-f3c8-4ec5-9a6a-cd90a46634b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9464, 0.0113],\n",
      "         [0.5183, 0.9807],\n",
      "         [0.6545, 0.4144]],\n",
      "\n",
      "        [[0.9464, 0.0113],\n",
      "         [0.5183, 0.9807],\n",
      "         [0.6545, 0.4144]],\n",
      "\n",
      "        [[0.9464, 0.0113],\n",
      "         [0.5183, 0.9807],\n",
      "         [0.6545, 0.4144]],\n",
      "\n",
      "        [[0.9464, 0.0113],\n",
      "         [0.5183, 0.9807],\n",
      "         [0.6545, 0.4144]]])\n",
      "tensor([[[0.0696, 0.0696],\n",
      "         [0.4648, 0.4648],\n",
      "         [0.4491, 0.4491]],\n",
      "\n",
      "        [[0.0696, 0.0696],\n",
      "         [0.4648, 0.4648],\n",
      "         [0.4491, 0.4491]],\n",
      "\n",
      "        [[0.0696, 0.0696],\n",
      "         [0.4648, 0.4648],\n",
      "         [0.4491, 0.4491]],\n",
      "\n",
      "        [[0.0696, 0.0696],\n",
      "         [0.4648, 0.4648],\n",
      "         [0.4491, 0.4491]]])\n",
      "tensor([[[0.6265, 0.9411],\n",
      "         [0.6265, 0.9411],\n",
      "         [0.6265, 0.9411]],\n",
      "\n",
      "        [[0.6265, 0.9411],\n",
      "         [0.6265, 0.9411],\n",
      "         [0.6265, 0.9411]],\n",
      "\n",
      "        [[0.6265, 0.9411],\n",
      "         [0.6265, 0.9411],\n",
      "         [0.6265, 0.9411]],\n",
      "\n",
      "        [[0.6265, 0.9411],\n",
      "         [0.6265, 0.9411],\n",
      "         [0.6265, 0.9411]]])\n"
     ]
    }
   ],
   "source": [
    "a =     torch.ones(4, 3, 2)\n",
    "\n",
    "b = a * torch.rand(   3, 2) # 第三和第二维与a相同，没有第一维\n",
    "print(b)\n",
    "\n",
    "c = a * torch.rand(   3, 1) # 第三维为1， 第二维与a相同\n",
    "print(c)\n",
    "\n",
    "d = a * torch.rand(   1, 2) # 第三维与a相同，第二维为1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f0812-c816-43e4-8dab-aee220c2d3ee",
   "metadata": {},
   "source": [
    "仔细看上面的每一个张量的值：\n",
    "\n",
    "- 创建 `b` 的乘法运算是在 `a` 的每一\"层\"上进行的。\n",
    "\n",
    "- 对于 `c`，该操作在 `a` 的每一层和每一行都被广播了--每一个3元素的列都是相同的。\n",
    "\n",
    "- 对于d，我们把它换了一下--现在每一行都是相同的，跨越层和列。\n",
    "\n",
    "有关广播的更多信息，请参见PyTorch的[相关文档](https://pytorch.org/docs/stable/notes/broadcasting.html)。\n",
    "\n",
    "下面是一些尝试广播会失败的例子："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2945f771-af7d-4cc3-86b6-6d1d3f7aa197",
   "metadata": {},
   "source": [
    "> **小贴士**\n",
    ">\n",
    "> 下面的每个单元格都会抛出 run-time error。这么做是故意的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43747838-ade9-45fb-90c2-54ed2409ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(4, 3, 2)\n",
    "\n",
    "b = a * torch.rand(4, 3)  # 维度必须从尾到头匹配\n",
    "\n",
    "c = a * torch.rand(2, 3) # 第二和第三维均不相同\n",
    "\n",
    "d = a * torch.rand((0, )) # 空张量不能广播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3111e97-a217-4e04-9cfa-f63335bc39cc",
   "metadata": {},
   "source": [
    "## More Math with Tensors\n",
    "\n",
    "PyTorch张量有超过三百种可以对其进行的运算。\n",
    "\n",
    "下面是一些主要的运算中的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "488a2e4b-9ff4-43f4-b13f-df73f2fd64e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common functions:\n",
      "tensor([[0.0156, 0.0921, 0.0792, 0.3893],\n",
      "        [0.6054, 0.3429, 0.1310, 0.9869]])\n",
      "tensor([[-0., 1., 1., -0.],\n",
      "        [-0., -0., 1., -0.]])\n",
      "tensor([[-1.,  0.,  0., -1.],\n",
      "        [-1., -1.,  0., -1.]])\n",
      "tensor([[-0.0156,  0.0921,  0.0792, -0.3893],\n",
      "        [-0.5000, -0.3429,  0.1310, -0.5000]])\n",
      "\n",
      "Sine and arcsine:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n",
      "\n",
      "Bitwise XOR:\n",
      "tensor([3, 2, 1])\n",
      "\n",
      "Broadcasted,  equality comparison:\n",
      "tensor([[ True, False],\n",
      "        [False, False]])\n",
      "\n",
      "Reduction ops:\n",
      "tensor(4.)\n",
      "4.0\n",
      "tensor(2.5000)\n",
      "tensor(1.2910)\n",
      "tensor(24.)\n",
      "tensor([1, 2])\n",
      "\n",
      "Vectors & Matrices:\n",
      "tensor([ 0.,  0., -1.])\n",
      "tensor([[0.7765, 0.3534],\n",
      "        [0.7016, 0.6826]])\n",
      "tensor([[2.3296, 1.0602],\n",
      "        [2.1048, 2.0479]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.6538, -0.7566],\n",
      "        [-0.7566,  0.6538]]),\n",
      "S=tensor([3.8390, 0.6614]),\n",
      "V=tensor([[-0.8116, -0.5842],\n",
      "        [-0.5842,  0.8116]]))\n"
     ]
    }
   ],
   "source": [
    "# 一般函数\n",
    "a = torch.rand(2, 4) * 2 - 1\n",
    "print('Common functions:')\n",
    "print(torch.abs(a)) # 绝对值\n",
    "print(torch.ceil(a)) # 向上取整\n",
    "print(torch.floor(a)) # 想下取整\n",
    "print(torch.clamp(a, -0.5, 0.5)) # 限幅，将a的取值限制在-0.5 0.5之间\n",
    "\n",
    "# 三角函数和它们的反函数\n",
    "angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi /4])\n",
    "sines = torch.sin(angles)\n",
    "inverses = torch.asin(sines)\n",
    "print('\\nSine and arcsine:')\n",
    "print(angles)\n",
    "print(sines)\n",
    "print(inverses)\n",
    "\n",
    "# 比特运算\n",
    "print('\\nBitwise XOR:')\n",
    "b = torch.tensor([1, 5, 11])\n",
    "c = torch.tensor([2, 7, 10])\n",
    "print(torch.bitwise_xor(b, c))\n",
    "\n",
    "# 比较\n",
    "print('\\nBroadcasted,  equality comparison:')\n",
    "d = torch.tensor([[1., 2.], [3., 4.]])\n",
    "e = torch.ones(1, 2) # 很多比较操作都支持广播！\n",
    "print(torch.eq(d, e)) # 返回一个布尔型的张量\n",
    "\n",
    "# 消减\n",
    "print('\\nReduction ops:')\n",
    "print(torch.max(d)) # 返回一个单值张量\n",
    "print(torch.max(d).item()) # 从返回的张量抽取数值\n",
    "print(torch.mean(d)) # 求张量均值\n",
    "print(torch.std(d)) # 标准差\n",
    "print(torch.prod(d)) # 所有数字相乘\n",
    "print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # 过滤唯一元素\n",
    "\n",
    "# 向量和线性代数运算\n",
    "v1 = torch.tensor([1., 0., 0.]) # x单位向量\n",
    "v2 = torch.tensor([0., 1., 0.]) # y单位向量\n",
    "m1 = torch.rand(2, 2) # 随机数矩阵\n",
    "m2 = torch.tensor([[3., 0.], [0., 3.]]) # 3倍特征矩阵\n",
    "\n",
    "print('\\nVectors & Matrices:')\n",
    "print(torch.cross(v2, v1)) # z单位向量的负(v1 x v2 == -v2 x v1)\n",
    "print(m1)\n",
    "m3 = torch.matmul(m1, m2)\n",
    "print(m3) # 3乘m1\n",
    "print(torch.svd(m3)) # 奇异值分解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc94079-82b1-4519-9dfd-4104170e210f",
   "metadata": {},
   "source": [
    "这只是一个小的运算样例。更多的细节和完整的数学函数清单，请看[文档](https://pytorch.org/docs/stable/torch.html#math-operations)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7635dba6-99ee-4c3b-b924-5c1614ad4d9b",
   "metadata": {},
   "source": [
    "## Altering Tensors in Place\n",
    "\n",
    "大多数对张量的二元运算将返回第三个新张量。当我们有 `c = a * b`（其中 `a` 和 `b` 是张量），新的张量 `c` 将开辟一个与其他张量不同的内存区域。\n",
    "\n",
    "不过，有些时候你可能希望就地改变张量--例如，如果你正在做一个可以丢弃中间值的元素小计算。为此，大多数数学函数都有一个带有下划线（`_`）的版本，可以就地改变一个张量。\n",
    "\n",
    "例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ab0ae2c-9e5c-47c9-af8c-e9523074dc5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "\n",
      "b:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('a:')\n",
    "print(a)\n",
    "print(torch.sin(a))   # 这个运算在内存中创建了一个行的张量\n",
    "print(a)              # a本身并未改变\n",
    "\n",
    "b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('\\nb:')\n",
    "print(b)\n",
    "print(torch.sin_(b))  # 注意下划线\n",
    "print(b)              # b已经改变了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c885fc-0c55-485a-892f-8904792670df",
   "metadata": {},
   "source": [
    "对于算数运算，也是一样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d60a79fd-b94e-48b2-a577-da120bebf7af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.9413, 0.4460],\n",
      "        [0.9289, 0.6293]])\n",
      "\n",
      "After adding:\n",
      "tensor([[1.9413, 1.4460],\n",
      "        [1.9289, 1.6293]])\n",
      "tensor([[1.9413, 1.4460],\n",
      "        [1.9289, 1.6293]])\n",
      "tensor([[0.9413, 0.4460],\n",
      "        [0.9289, 0.6293]])\n",
      "\n",
      "After multiplying\n",
      "tensor([[0.8861, 0.1989],\n",
      "        [0.8628, 0.3960]])\n",
      "tensor([[0.8861, 0.1989],\n",
      "        [0.8628, 0.3960]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "\n",
    "print('Before:')\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter adding:')\n",
    "print(a.add_(b))\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter multiplying')\n",
    "print(b.mul_(b))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d39ed6-2794-411d-af20-605373eec410",
   "metadata": {},
   "source": [
    "请注意，这些就地算术函数是 `torch.Tensor` 对象上的方法，而不是像许多其他函数（如`torch.sin()` ）那样附属于 `torch` 模块。从 `a.add_(b)` 可以看出，调用的张量是被就地改变的。\n",
    "\n",
    "还有一个选择是将计算的结果放在一个现有的、分配的张量中。到目前为止，我们所看到的许多方法和函数--包括创建方法！都有一个 `out` 参数，它可以让你指定一个张量来接收输出！- 有一个out参数，让你指定一个张量来接收输出。如果输出张量是正确的形状和 `dtype`，这不会进行新的内存分配："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3412f01f-8619-4153-92b0-bb9dc4091105",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0.4869, 0.6557],\n",
      "        [0.5373, 0.6492]])\n",
      "tensor([[0.1399, 0.9280],\n",
      "        [0.1986, 0.1779]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "c = torch.zeros(2, 2)\n",
    "old_id = id(c)\n",
    "\n",
    "print(c)\n",
    "d = torch.matmul(a, b, out=c)\n",
    "print(c)  # c中的值已经改变了\n",
    "\n",
    "assert c is d  # 测试c和d是同一个对象，不仅仅比较它们的值\n",
    "assert id(c) == old_id  # 确保新的c和旧的是同一个对象\n",
    "\n",
    "torch.rand(2, 2, out=c)  # 对于创建也同样生效！\n",
    "print(c)                 # c又一次改变了\n",
    "assert id(c) == old_id   # 仍然是同一个对象！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817fecc-d963-4707-b790-8c2806de558b",
   "metadata": {},
   "source": [
    "## Copying Tensors\n",
    "\n",
    "和Python中的对象一样，将张量赋值给一个变量会使该变量成为张量的*标签*，而不是复制它。比如说:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9005cb74-c364-4a4d-bbcb-f4e4c7cddb32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a\n",
    "\n",
    "a[0][1] = 561  # 我们改变a\n",
    "print(b)       # b的值也发生了改变"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3df568-7dd6-4bbe-addf-1e9219d688f9",
   "metadata": {},
   "source": [
    "但是如果你想要一个单独的数据副本来工作呢？`clone()`方法就是为你而设的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82d4c8fe-7fcd-442b-adde-3e68f1fb1a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a.clone()\n",
    "\n",
    "assert b is not a  # 在内存中的不同对象\n",
    "print(torch.eq(a, b))  # 但是值依旧相等\n",
    "\n",
    "a[0][1] = 561  # a变了\n",
    "print(b)  # 但是b没改变"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff59d132-18f8-4fff-95ee-3093fa053f74",
   "metadata": {},
   "source": [
    "**当调用在使用 `clone()` 时，有一件重要的事情需要注意**。如果你的源张量启用了autograd，那么克隆的也会启用。**这将在关于autograd的视频中更深入地介绍**，但如果你想了解更多的细节，请继续。\n",
    "\n",
    "在许多情况下，你需要这么做。例如，如果你的模型在其forward()方法中有多个计算路径，并且原始张量和其克隆对模型的输出都有贡献，那么为了实现模型学习，你希望对两个张量都打开autograd。如果你的源张量已经启用了autograd（如果它是一组学习权重或者是从涉及权重的计算中派生出来的，那么它通常会启用），那么你会得到你想要的结果。\n",
    "\n",
    "另一方面，如果你在做一个计算，原始张量和它的克隆都不需要跟踪梯度，那么只要关闭源张量的autograd就可以了。\n",
    "\n",
    "不过还有第三种情况：想象一下你在模型的 `forward()` 函数中进行计算，梯度在默认情况下是打开的，但是你想在中途抽出一些值来生成一些指标。在这种情况下，你不希望你的源张量的克隆副本跟踪梯度--关闭autograd的历史跟踪后，性能会得到改善。为此，你可以在源张量上使用`.detach()` 方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d33061d-f40a-449e-ab3a-ad6c988cf345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6366, 0.2301],\n",
      "        [0.9151, 0.5056]], requires_grad=True)\n",
      "tensor([[0.6366, 0.2301],\n",
      "        [0.9151, 0.5056]], grad_fn=<CloneBackward0>)\n",
      "tensor([[0.6366, 0.2301],\n",
      "        [0.9151, 0.5056]])\n",
      "tensor([[0.6366, 0.2301],\n",
      "        [0.9151, 0.5056]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2, requires_grad=True)  # 打开autograd\n",
    "print(a)\n",
    "\n",
    "b = a.clone()\n",
    "print(b)\n",
    "\n",
    "c = a.detach().clone()\n",
    "print(c)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85500d45-d888-49dc-a066-c0dd362bba72",
   "metadata": {},
   "source": [
    "这里发生了什么？\n",
    "\n",
    "- 我们创建了 `a` 并将它的 `requires_grad=True` 打开。**我们所讲的目前还没涉及到这一操作，但是在autogard单元中会涉及到。**\n",
    "- 当我们打印 `a`，它告诉我们属性 `requires_grad=True`，这意味着autogard和计算历史追踪已经打开。\n",
    "- 我们克隆 `a` 并且用 `b` 表示。当我们打印 `b`，我们可以看到它正在追踪它的计算历史，它继承了 `a` 的autograd，并且加入了计算历史。\n",
    "- 我们把 `a` 克隆到 `c`，但是我们首先调用了 `detach()`。\n",
    "- 打印 `c`，我们没有看到计算历史，也没看到 `requires_grad=True`。\n",
    "\n",
    "`detach()` 方法将张量从其计算历史中分离出来。它说，\"接下来的事情都是在关闭autograd的情况下进行的\"。它是在不改变 `a` 的情况下进行的, 你可以看到，当我们在最后再次打印 `a` 时，它保留了 `requires_grad=True` 的属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd25695a-ecdc-440d-b1eb-9e3a145538f8",
   "metadata": {},
   "source": [
    "## Moving to GPU\n",
    "\n",
    "PyTorch的主要优势之一在于它在兼容CUDA的Nvidia GPU上的强大加速能力。(\"CUDA \"是Compute Unified Device Architecture的缩写，它是Nvidia的并行计算平台。) 到目前为止，我们所做的一切都是在CPU上进行的。我们怎样才能将这些操作转移到更快的硬件上呢？\n",
    "\n",
    "首先，我们应该通过 `is_available()` 方法检查GPU是否可用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eba9e752-92b7-43cb-9ca1-a72e9d540cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们有GPU！\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('我们有GPU！')\n",
    "else:\n",
    "    print('很遗憾，只有CPU。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988395b7-f478-48cb-86cd-a16fd1ffd6b0",
   "metadata": {},
   "source": [
    "一但我们确定了一个或多个GPU可用，我们就需要将我们的数据放到GPU能看到的地方。你的CPU会在你电脑上的RAM上进行运算。你的GPU有专用的内存连接到它。每当你想在一个设备上进行计算时，你必须将该计算所需的所有数据转移到该设备可访问的内存中。(俗话说，\"把数据移到GPU可访问的内存 \"被简称为 \"把数据移到GPU\")。\n",
    "\n",
    "有多种方法可以将你的数据带到你的目标设备上。你可以在创建时进行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52ac21ef-7754-45e0-8ccf-a46d3121967d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3344, 0.2640],\n",
      "        [0.2119, 0.0582]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_rand = torch.rand(2, 2, device='cuda')\n",
    "    print(gpu_rand)\n",
    "else:\n",
    "    print('很遗憾，只有CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27816c78-1cae-4dad-ad31-c2e343050bf9",
   "metadata": {},
   "source": [
    "默认情况下，新的张量会被创建在CPU上，所以当我们想将张量创建到GPU上时，我们需要指定可选参数 `device`。当我们打印新的张量时，PyTorch会告诉我们它在哪个设备上（如果不在CPU上）。\n",
    "\n",
    "你可以用 `torch.cuda.device_count()` 来查询GPU的数量。如果你的GPU数量大于1，你可以通过指定下标来指定某一个：`device='cuda:0'`, `device='cuda:1'`,等等。\n",
    "\n",
    "作为一种编码实践，用字符串常量来指定我们的设备是不好的。在一个理想情况下，无论你是在CPU还是GPU硬件上，你的代码都应该表现得很鲁棒。你可以通过创建一个可以传递给你的张量的设备句柄，而不是一个字符串来实现这一点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9abf9f95-11a5-4eeb-b9ad-af914f27356c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:3\n",
      "tensor([[0.3344, 0.2640],\n",
      "        [0.2119, 0.0582]], device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda:3')\n",
    "else:\n",
    "    my_device = torch.device('cup')\n",
    "print('Device: {}'.format(my_device))\n",
    "\n",
    "x = torch.rand(2, 2, device=my_device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926c529-9921-4b7b-aa8f-712a5e223e03",
   "metadata": {},
   "source": [
    "如果你已经有了一个在某一设备上的张量，你可以使用 `to()` 方法来将它移动到另一个设备。下面的代码在CPU上创建了一个张量，并把它移到你在前一个单元格中获得的设备句柄上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "474bc75b-69e7-4ca5-bc9c-69bce64eb66d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = torch.rand(2, 2)\n",
    "y = y.to(my_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a7c72-3d46-4750-bfd6-84fe479499ff",
   "metadata": {},
   "source": [
    "重要的是要知道，为了进行涉及两个或更多张量的计算，所有的张量必须在同一个设备上。无论你是否有一个可用的GPU设备，下面的代码都会引发一个运行时错误："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2387c73-1202-441d-b862-46d29c64e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2, device='gpu')\n",
    "z = x + y # 将会抛出异常"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d52e68-2d55-4bde-a916-c28072a00ee3",
   "metadata": {},
   "source": [
    "## Manipulating Tensor Shapes\n",
    "\n",
    "有时，你需要改变你的张量的形状。下面，我们来看看几种常见的情况，以及如何处理它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42559dc-0468-470d-9011-45b0c71c8cda",
   "metadata": {},
   "source": [
    "### Changing the Number of Dimensions\n",
    "\n",
    "你可能需要改变维数的一种情况是将单个输入实例传递给你的模型。PyTorch模型通常希望有成批的输入。\n",
    "\n",
    "例如，有一个在3 x 226 x 226图像上工作的模型--一个226像素的正方形，有3个颜色通道。当你加载和转换它时，你会得到一个形状的张量 `(3, 226, 226)`。但是，你的模型希望输入的是形状 `(N, 3, 226, 226)`，其中N是一个批次中的图像数量。那么，你如何制作一个批次的图像呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "951923f5-a99c-4459-b567-9b23f9cf8452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 226, 226)\n",
    "b = a.unsqueeze(0)  # 在0维扩充一个为 1 的维度\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c48ef86-e887-457b-9827-8a64b71b490f",
   "metadata": {},
   "source": [
    "`unsqueeze()` 方法添加了一个范围为1的维度，`unsqueeze(0)` 将其作为新的第0个维度添加--现在你有了一个批量为1的维度!\n",
    "\n",
    "那么，如果这就是unsqueezing？我们所说的squeezing是什么意思？我们正在利用这样一个事实：任何范围为1的维度都不会改变张量中的元素数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39410746-fe68-46ce-96e0-40b7b444e1dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[0.3442]]]]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.rand(1, 1, 1, 1, 1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65281de-106a-487c-95f9-773240d3d972",
   "metadata": {},
   "source": [
    "继续上面的例子，假设模型对每个输入的输出是20个元素的向量。那么你会期望输出的形状是（N，20），其中N是输入批次中的实例数。这意味着，对于我们的单一输入批次，我们将得到一个形状为（1，20）的输出。\n",
    "\n",
    "如果你想用这个输出做一些非批处理的计算，只是想得到一个20元素的向量，该怎么做？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d15895e-088e-45af-906c-00383477b9e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "tensor([[0.9211, 0.6042, 0.7966, 0.6166, 0.6064, 0.9906, 0.0350, 0.0411, 0.7051,\n",
      "         0.5522, 0.2367, 0.5497, 0.1662, 0.2961, 0.9811, 0.6110, 0.1597, 0.4791,\n",
      "         0.5845, 0.8477]])\n",
      "torch.Size([20])\n",
      "tensor([0.9211, 0.6042, 0.7966, 0.6166, 0.6064, 0.9906, 0.0350, 0.0411, 0.7051,\n",
      "        0.5522, 0.2367, 0.5497, 0.1662, 0.2961, 0.9811, 0.6110, 0.1597, 0.4791,\n",
      "        0.5845, 0.8477])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, 20)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "\n",
    "b = a.squeeze(0)\n",
    "print(b.shape)\n",
    "print(b)\n",
    "\n",
    "c = torch.rand(2, 2)\n",
    "print(c.shape)\n",
    "\n",
    "d = c.squeeze(0)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8113f8d6-b3d6-4203-8774-2fce5652fb4e",
   "metadata": {},
   "source": [
    "你可以从shape中看到，我们的二维张量现在是一维的，如果你仔细观察上面单元格的输出，你会发现打印出的 `a` 显示了一组 “额外的” 方括号 `[]`，因为有一个额外的维度。\n",
    "\n",
    "你只能 `squeezing()` 大小为1的维度。请看上面，我们试图在 `c` 中 `squeezing` 一个2的维度，结果得到的是与开始相同的形状。对 `squeeze()` 和 `unsqueeze()` 的调用只能作用于范围为1的维度，否则会改变张量的元素数量。\n",
    "\n",
    "你可能使用 `unsqueeze()` 的另一个地方是为了缓解广播。回顾上面的例子，我们有以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3fb13ec-02bb-494e-ad33-5815ddbee5de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7074, 0.7074],\n",
      "         [0.9344, 0.9344],\n",
      "         [0.2187, 0.2187]],\n",
      "\n",
      "        [[0.7074, 0.7074],\n",
      "         [0.9344, 0.9344],\n",
      "         [0.2187, 0.2187]],\n",
      "\n",
      "        [[0.7074, 0.7074],\n",
      "         [0.9344, 0.9344],\n",
      "         [0.2187, 0.2187]],\n",
      "\n",
      "        [[0.7074, 0.7074],\n",
      "         [0.9344, 0.9344],\n",
      "         [0.2187, 0.2187]]])\n"
     ]
    }
   ],
   "source": [
    "a =     torch.ones(4, 3, 2)\n",
    "c = a * torch.rand(   3, 1)  # 第三维为 1 ，第二维与 a 相同\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d304d7-9325-4f49-9b9e-c5f86f5b212c",
   "metadata": {},
   "source": [
    "这样做的效果是在维度0和维度2上进行广播操作，导致随机的3×1张量被 `a` 中的每一个3元素的列进行元素相乘。\n",
    "\n",
    "如果随机向量只是3元素的向量呢？我们就会失去做广播的能力，因为根据广播规则，最终的尺寸不会匹配。 `unsqueeze()` 会来拯救我们："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c4bf4ae-d6a0-475f-b38b-73a44d52ac88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "tensor([[[0.1061, 0.1061],\n",
      "         [0.4142, 0.4142],\n",
      "         [0.8224, 0.8224]],\n",
      "\n",
      "        [[0.1061, 0.1061],\n",
      "         [0.4142, 0.4142],\n",
      "         [0.8224, 0.8224]],\n",
      "\n",
      "        [[0.1061, 0.1061],\n",
      "         [0.4142, 0.4142],\n",
      "         [0.8224, 0.8224]],\n",
      "\n",
      "        [[0.1061, 0.1061],\n",
      "         [0.4142, 0.4142],\n",
      "         [0.8224, 0.8224]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4, 3, 2)\n",
    "b = torch.rand(3)   # 尝试 a * b 将会返回一个运行时错误\n",
    "c = b.unsqueeze(1)  # 改变一个二维张量，在末尾新加一个维度\n",
    "print(c.shape)\n",
    "print(a * c)  # 广播再次生效了！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65cba4-110e-4997-b578-b35789fc5754",
   "metadata": {},
   "source": [
    "`squeeze()`和`unsqueeze()` 方法都有一个原地操作版本，`squeeze_()` 和 `unsqueeze_()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77d1bc3b-4a60-4ddc-b887-62afe2c29a68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 266, 266])\n",
      "torch.Size([1, 3, 266, 266])\n"
     ]
    }
   ],
   "source": [
    "batch_me = torch.rand(3, 266, 266)\n",
    "print(batch_me.shape)\n",
    "batch_me.unsqueeze_(0)\n",
    "print(batch_me.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbdbf29-89fa-47f0-97af-b0e1abe458ec",
   "metadata": {},
   "source": [
    "有时你会想更彻底地改变张量的形状，同时仍然保留元素的数量和它们的内容。一种情况是在模型的卷积层和模型的线性层之间的层--这在图像分类模型中很常见。卷积核会产生一个形状x宽度x高度的输出张量，但是下面的线性层希望得到一个一维的输入。`reshape()` 会帮你做这个，只要你要求的维度产生的元素数量与输入张量相同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7750bda0-111f-4b49-a669-521411a2d694",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 20, 20])\n",
      "torch.Size([2400])\n",
      "torch.Size([2400])\n"
     ]
    }
   ],
   "source": [
    "output3d = torch.rand(6, 20, 20)\n",
    "print(output3d.shape)\n",
    "\n",
    "input1d = output3d.reshape(6 * 20 *  20)\n",
    "print(input1d.shape)\n",
    "\n",
    "# 也可以把它作为torch模块的一个方法来调用:\n",
    "print(torch.reshape(output3d, (6 * 20 * 20,)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e385a1-1562-4a6d-8390-7493a01c20db",
   "metadata": {},
   "source": [
    "> **小贴士**\n",
    ">\n",
    "> 上面单元格最后一行的`(6 * 20 * 20,)`参数是因为PyTorch在指定张量形状时希望输入是一个**元组**，但当形状是方法的第一个参数时，它允许我们偷懒，只使用一系列的整数。在这里，我们不得不添加括号和逗号，以说服方法，这实际上是一个单元素元组。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be7f3c9-d168-4a0c-8c71-f774cf2b8296",
   "metadata": {},
   "source": [
    "`reshape()`将返回一个要改变的张量上的*视图*，也就是说，一个单独的张量对象看同一个内存底层区域。这一点很重要：这意味着对源张量的任何改变都会反映在该张量的视图中，除非你`clone()` 它。\n",
    "\n",
    "有些情况，超出本介绍的范围，reshape()必须返回一个携带数据副本的张量。欲了解更多信息，请参见[文档](https://pytorch.org/docs/stable/torch.html#torch.reshape)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbaf1c0-054e-4bc6-a389-caa5327f9357",
   "metadata": {},
   "source": [
    "## NumPy Bridge\n",
    "\n",
    "在上面关于广播的章节中提到，PyTorch的广播与NumPy是兼容的--但PyTorch和NumPy之间的关系甚至比这更深。\n",
    "\n",
    "如果你现有的ML或科学代码中的数据存储在NumPy的ndarrays中，你可能希望将相同的数据转换为PyTorch的张量，无论是利用PyTorch的GPU加速，还是为建立机器学习模型。在ndarrays和PyTorch tensors之间切换很容易："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d73338c-0d1f-414c-91f2-4ee3b7699a88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numpy_array = np.ones((2, 3))\n",
    "print(numpy_array)\n",
    "\n",
    "pytorch_tensor = torch.from_numpy(numpy_array)\n",
    "print(pytorch_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f992911f-bd43-4221-8fe4-7303ea885f6f",
   "metadata": {},
   "source": [
    "PyTorch创建了一个与NumPy数组相同形状，相同数据的张量，保留的数据类型为NumPy默认的64位浮点数类型。\n",
    "\n",
    "反向的转换也很容易："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8352f9c3-4d45-422e-b6e0-cd7e10968541",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7434, 0.4376, 0.7223],\n",
      "        [0.4210, 0.4927, 0.8151]])\n",
      "[[0.74341977 0.43758988 0.7222695 ]\n",
      " [0.42100137 0.49266487 0.81514764]]\n"
     ]
    }
   ],
   "source": [
    "pytorch_rand = torch.rand(2, 3)\n",
    "print(pytorch_rand)\n",
    "\n",
    "numpy_rand = pytorch_rand.numpy()\n",
    "print(numpy_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa61d3-5503-47df-84e1-e50d74861551",
   "metadata": {},
   "source": [
    "重要的是要知道，这些转换后的对象与它们的源对象使用相同的底层内存，这意味着一个对象的变化会反映在另一个对象上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3039641-7c9b-4532-8347-7129749b8768",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1., 23.,  1.]], dtype=torch.float64)\n",
      "[[ 0.74341977  0.43758988  0.7222695 ]\n",
      " [ 0.42100137 17.          0.81514764]]\n"
     ]
    }
   ],
   "source": [
    "numpy_array[1, 1] = 23\n",
    "print(pytorch_tensor)\n",
    "\n",
    "pytorch_rand[1, 1] = 17\n",
    "print(numpy_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3ffb3-470e-45fb-9469-9eb2cd13ece8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpth",
   "language": "python",
   "name": "jpth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
