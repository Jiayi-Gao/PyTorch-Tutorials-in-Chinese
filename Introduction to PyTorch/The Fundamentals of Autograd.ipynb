{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2204c1de-6b53-4eb3-9b44-6a4f7d6217de",
   "metadata": {},
   "source": [
    "# THE FUNDAMENTALS OF AUTOGRAD\n",
    "\n",
    "PyTorch的Autograd功能是PyTorch在构建机器学习项目时灵活、快速的原因之一。它允许在一个复杂的计算中快速和便捷地计算多个偏导数（也被称为梯度）。这种操作是基于反向传播的神经网络学习的核心。\n",
    "\n",
    "autograd的强大之处在于它在运行时动态追踪你的计算，这意味着如果你的模型有决策分支，或者在运行时才知道其长度的循环，计算仍然会被正确追踪，你会得到正确的梯度来驱动学习。这一点，再加上你的模型是用Python构建的，比那些依靠静态分析更严格的结构化模型来计算梯度的框架要灵活得多。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7108614-5b17-4d20-9e39-18e9a2975de3",
   "metadata": {},
   "source": [
    "## What Do We Need Autograd For?\n",
    "\n",
    "一个机器学习的模型就是一个带有输入和输出的函数。在这一节的讨论中，我们把带有$x_i$元素的一个i维向量 $\\vec x$作为输入。然后，我们可以将模型M表示为输入的矢量值函数： $\\vec y = \\vec M (\\vec x)$。（我们把M的输出值当作一个向量，因为一般来说，一个模型可以有任何数量的输出。）\n",
    "\n",
    "由于我们主要是在训练的背景下讨论autograd，我们感兴趣的输出将是模型的损失。损失函数$L(\\vec y) = L(\\vec M(\\vec x))$是一个模型输出的单值的标量函数。这个函数表示我们的模型预测与某一特定输入的理想输出有多大的偏差。注意：在这之后，我们通常会在上下文明确的省略矢量符号--例如，$y$代表$\\vec y$。\n",
    "\n",
    "在训练一个模型时，我们要使损失最小化。在理想化的完美模型的情况下，这意味着调整其学习权重--也就是函数的可调参数--使所有输入的损失为零。在现实世界中，这意味着一个反复调整学习权重的过程，直到我们看到模型在各种输入下都可以得到一个可容忍的损失。\n",
    "\n",
    "我们如何确定在多大程度上和哪个方向上推动权重？我们想使损失*最小化*，这意味着使其相对于输入的第一次导数等于0： $\\frac{\\partial L}{\\partial x} = 0$。\n",
    "\n",
    "不过，回顾一下，损失不是直接来自输入，而是模型输出的一个函数（是输入的一个函数），$\\frac {\\partial L}{\\partial x} = \\frac {\\partial L(\\vec y)}{\\partial x}$，根据链式求导法则，我们有$\\frac {\\partial L(\\vec y)}{\\partial x} = \\frac {\\partial L}{\\partial y} \\frac {\\partial y}{\\partial x} = \\frac {\\partial L}{\\partial y} \\frac {\\partial M(x)}{\\partial x}$。\n",
    "\n",
    "$\\frac {\\partial M(x)}{\\partial x}$ 是事情变复杂的地方。模型输出的偏导数与它的输入有关，如果我们再次使用链式法则扩展表达式，这将涉及到对每一个乘法学习权重、每一个激活函数和模型中每一个其他数学变换的许多局部偏导。每个这样的偏导的完整表达式是通过计算图的每个可能路径的局部梯度的乘积之和，这些路径的终点是我们试图计算其梯度的变量。\n",
    "\n",
    "特别地，我们感兴趣的是学习权重的梯度，它们告诉我们每个权重向*什么方向改变*来使损失趋近于0。\n",
    "\n",
    "由于局部导数（每一个都相对于模型计算图的一个单独路径）的数量将会随着神经网络的深度呈指数级增长，它们计算的复杂度也是如此。这就是autograd介入的地方，它会追踪每一个计算的历史。PyTorch模型中的每个计算张量都带有其输入张量和用于创建张量的函数的历史。再加上旨在作用于张量的PyTorch函数都有一个内置的实现来计算自己的导数，这大大加快了学习所需的局部导数的计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e15b5ee-9b43-494c-8bef-706a84f660f5",
   "metadata": {},
   "source": [
    "## A Simple Example\n",
    "\n",
    "上面是一些理论，但是在实践中使用autograd是什么样的呢？\n",
    "\n",
    "让我们通过一个简单的样例看看。首先，首先，我们要做一些导入，让我们把结果以图的形式展示出来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757144ef-ca7d-4ec5-9d79-1cfafda8a862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c839a689-f689-4dca-80ce-ac515dd83948",
   "metadata": {},
   "source": [
    "接下来，我们将创建一个输入张量，该张量填充在区间 $[0,2\\pi]$ 内均匀分布的数值，并指定 `requires_grad=True`。（和大多数创建张量的函数一样，`torch.linspace()` 接受一个可选的 `requires_grad` 选项）。设置这个标志意味着在接下来的每一次计算中，autograd将在该计算的输出张量中积累计算的历史。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4999df-57ca-40a6-93cc-0839de6bdf2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.2618, 0.5236, 0.7854, 1.0472, 1.3090, 1.5708, 1.8326, 2.0944,\n",
      "        2.3562, 2.6180, 2.8798, 3.1416, 3.4034, 3.6652, 3.9270, 4.1888, 4.4506,\n",
      "        4.7124, 4.9742, 5.2360, 5.4978, 5.7596, 6.0214, 6.2832],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864fc66b-be21-47ec-89f0-607c8686de54",
   "metadata": {},
   "source": [
    "接下来，我们将进行一次计算，并根据其输入绘制其输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46ba4b29-02b3-4edc-aa32-3f7ff25108dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f805ab70a90>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = torch.sin(a)\n",
    "plt.plot(a.detach(), b.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e706f98d-85d7-491f-9ea8-4cb0ca47ba30",
   "metadata": {},
   "source": [
    "我们在仔细的看一下张量 `b`。当我们打印它，我们看到了一个指标，它正在跟踪其计算历史："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41376dc4-3601-4df5-a4a2-7ad9f593342a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000e+00,  2.5882e-01,  5.0000e-01,  7.0711e-01,  8.6603e-01,\n",
      "         9.6593e-01,  1.0000e+00,  9.6593e-01,  8.6603e-01,  7.0711e-01,\n",
      "         5.0000e-01,  2.5882e-01, -8.7423e-08, -2.5882e-01, -5.0000e-01,\n",
      "        -7.0711e-01, -8.6603e-01, -9.6593e-01, -1.0000e+00, -9.6593e-01,\n",
      "        -8.6603e-01, -7.0711e-01, -5.0000e-01, -2.5882e-01,  1.7485e-07],\n",
      "       grad_fn=<SinBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8004afb2-d322-404d-859f-fc56eb471df9",
   "metadata": {},
   "source": [
    "这个 `grad_fn` 给了我们一个提示，当我们执行反向传播步骤并计算梯度时，我们需要为这个张量的所有输入计算$\\sin(x)$的梯度。\n",
    "\n",
    "让我们再进行一些计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ab9e84-c662-4a11-ad82-7fa3aea989a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000e+00,  5.1764e-01,  1.0000e+00,  1.4142e+00,  1.7321e+00,\n",
      "         1.9319e+00,  2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,\n",
      "         1.0000e+00,  5.1764e-01, -1.7485e-07, -5.1764e-01, -1.0000e+00,\n",
      "        -1.4142e+00, -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00,\n",
      "        -1.7321e+00, -1.4142e+00, -1.0000e+00, -5.1764e-01,  3.4969e-07],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([ 1.0000e+00,  1.5176e+00,  2.0000e+00,  2.4142e+00,  2.7321e+00,\n",
      "         2.9319e+00,  3.0000e+00,  2.9319e+00,  2.7321e+00,  2.4142e+00,\n",
      "         2.0000e+00,  1.5176e+00,  1.0000e+00,  4.8236e-01, -3.5763e-07,\n",
      "        -4.1421e-01, -7.3205e-01, -9.3185e-01, -1.0000e+00, -9.3185e-01,\n",
      "        -7.3205e-01, -4.1421e-01,  4.7684e-07,  4.8236e-01,  1.0000e+00],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = 2 * b\n",
    "print(c)\n",
    "\n",
    "d = c + 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3409b-179f-4c02-af6c-e76dd4e82e77",
   "metadata": {},
   "source": [
    "最后，让我们来计算一个单元素的输出。当你在一个没有参数的张量上调用 `.backward()` 时，它希望调用的张量只包含一个元素，就像计算损失函数时的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "417b6314-86b8-4a0d-978c-07384c825769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = d.sum()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a327fab-094d-459b-861e-23b9640fefb1",
   "metadata": {},
   "source": [
    "与我们的张量一起存储的每个 `grad_fn` 允许你通过其 `next_functions` 属性将计算一直追溯到其输入。我们可以看到，在 `d` 的这个属性上往下追溯，可以看到所有先前的张量的梯度函数。请注意，`a.grad_fn` 被报告为 `None` ，表明这是一个没有历史的函数的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7be81ed0-d283-41e2-8106-ad14dad6973a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\n",
      "<AddBackward0 object at 0x7f8052379bd0>\n",
      "((<MulBackward0 object at 0x7f80522805d0>, 0), (None, 0))\n",
      "((<SinBackward0 object at 0x7f805222f290>, 0), (None, 0))\n",
      "((<AccumulateGrad object at 0x7f80522805d0>, 0),)\n",
      "()\n",
      "\n",
      "c:\n",
      "<MulBackward0 object at 0x7f8052372310>\n",
      "\n",
      "b:\n",
      "<SinBackward0 object at 0x7f805222f290>\n",
      "\n",
      "a:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('d:')\n",
    "print(d.grad_fn)\n",
    "print(d.grad_fn.next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print('\\nc:')\n",
    "print(c.grad_fn)\n",
    "print('\\nb:')\n",
    "print(b.grad_fn)\n",
    "print('\\na:')\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00973244-7ed9-4df4-9d28-6268ab140bd3",
   "metadata": {},
   "source": [
    "有了所有这些机制，我们如何获得导数呢？我们可以在输出上调用 `backward()` 方法，并检查输入的 `grad` 属性以检查梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98797d2a-40de-4aaf-ad92-aa8e24fe4c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,  1.0000e+00,\n",
      "         5.1764e-01, -8.7423e-08, -5.1764e-01, -1.0000e+00, -1.4142e+00,\n",
      "        -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00, -1.7321e+00,\n",
      "        -1.4142e+00, -1.0000e+00, -5.1764e-01,  2.3850e-08,  5.1764e-01,\n",
      "         1.0000e+00,  1.4142e+00,  1.7321e+00,  1.9319e+00,  2.0000e+00])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8052152050>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "out.backward()\n",
    "print(a.grad)\n",
    "plt.plot(a.detach(), a.grad.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc137e-b672-4a20-b46e-62edfcb7f75d",
   "metadata": {},
   "source": [
    "回忆一下我们走到这里的计算步骤："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8171157b-deae-4c70-9c2d-d3d354134310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "b = torch.sin(a)\n",
    "c = 2 * b\n",
    "d = c + 1\n",
    "out = d.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54755bc5-2c33-4289-9093-f89465f7023a",
   "metadata": {},
   "source": [
    "添加一个常数，就像我们计算 `d` 那样，并不改变导数。这样，$c= 2 \\times b=2 \\times sin(a)$，其导数应该是 $2 \\times cos(a)$。看一下上面的图，这就是我们看到的情况。\n",
    "\n",
    "请注意，只有计算的叶子节点才会计算它们的梯度。如果你尝试 `print(c.grad)` 你会得到 `None`。在这个简单的例子中，只有输入是一个叶子节点，所以只有它有梯度计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2161d554-0ca9-4279-bf75-a02322356112",
   "metadata": {},
   "source": [
    "## Autograd in Training\n",
    "\n",
    "我们已经简单了解了autograd的工作原理，但当它被用于预定的目的时，它看起来如何？让我们定义一个小模型，并检查它在一个训练批次后的变化。首先，定义几个常量，我们的模型，以及一些输入和输出的替代："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0c4bc22-3b00-402d-aefe-180d3b4ee8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "DIM_IN = 1000\n",
    "HIDDEN_SIZE = 100\n",
    "DIM_OUT = 10\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        \n",
    "        self.layer1 = torch.nn.Linear(1000, 100)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(100, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    \n",
    "some_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False)\n",
    "ideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=False)\n",
    "\n",
    "model = TinyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c768d-9a63-49a1-b1a7-357bf18ff054",
   "metadata": {},
   "source": [
    "你可能会注意到一件事，我们从未为模型的层指定 `requires_grad=True`。在 `torch.nn.Module` 的子类中，我们假定要跟踪各层权重的梯度进行学习。\n",
    "\n",
    "如果我们看一下模型的层，我们可以检查权重的值，并验证是否还没有计算梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48f76d3d-05e1-4a1f-b87b-825cb22bb931",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0682,  0.0344,  0.0158,  0.0015,  0.0012,  0.0013,  0.0707, -0.0115,\n",
      "        -0.0758, -0.0405], grad_fn=<SliceBackward0>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.layer2.weight[0][0:10]) # 一个小切片\n",
    "print(model.layer2.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa6d3a1-0405-429a-a4a2-8476233cfade",
   "metadata": {},
   "source": [
    "让我们看看当我们运行一个训练批次时，情况会有什么变化。我们使用我们的预测和理想输出之间的欧氏距离的平方作为损失函数，并且我们将使用一个基本的随机梯度下降优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3729bf10-28ff-45d4-a832-dd375489bdb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(131.5259, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "prediction = model(some_input)\n",
    "\n",
    "loss = (ideal_output - prediction).pow(2).sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd7b8c-644a-402f-9f5c-69e0868ba010",
   "metadata": {},
   "source": [
    "现在我们调用 `loss.backward()` 并且看看发生了什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c0f0549-2540-4bdc-b188-c99117b85ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0682,  0.0344,  0.0158,  0.0015,  0.0012,  0.0013,  0.0707, -0.0115,\n",
      "        -0.0758, -0.0405], grad_fn=<SliceBackward0>)\n",
      "tensor([ 2.0845,  0.4690, -4.0173,  2.4279,  0.9756, -0.8618,  2.5182,  1.6739,\n",
      "        -2.2831,  1.1891])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(model.layer2.weight[0][0:10])\n",
    "print(model.layer2.weight.grad[0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b7200d4-2511-47e0-b8d6-2b258d1531a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.layer2.weight[0][0:10] - model.layer2.weight.grad[0][0:10] * 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622b1984-bfde-45fc-96f3-4bdb301bf805",
   "metadata": {},
   "source": [
    "我们可以看到，每个学习权重的梯度已经计算出来了，但是权重仍然没有变化，因为我们还没有运行优化器。优化器负责根据计算的梯度更新模型权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59f03932-5663-4501-9257-953c447346c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0703,  0.0339,  0.0198, -0.0009,  0.0002,  0.0021,  0.0682, -0.0131,\n",
      "        -0.0735, -0.0417], grad_fn=<SliceBackward0>)\n",
      "tensor([ 2.0845,  0.4690, -4.0173,  2.4279,  0.9756, -0.8618,  2.5182,  1.6739,\n",
      "        -2.2831,  1.1891])\n"
     ]
    }
   ],
   "source": [
    "optimizer.step()\n",
    "print(model.layer2.weight[0][0:10])\n",
    "print(model.layer2.weight.grad[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec5f464-8e36-49c6-85cd-e8770561c151",
   "metadata": {},
   "source": [
    "你应该看到了 `layer2` 的权重发生了变化。\n",
    "\n",
    "关于这个过程，有一件很重要的事情： 在调用 `optimizer.step()` 之后，你需要调用`optimizer.zero_grad()`，否则每次运行loss.backward()时，学习权重上的梯度会累积起来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "732b827d-29f5-4c1e-8d43-8d9b7f70b764",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.0845,  0.4690, -4.0173,  2.4279,  0.9756, -0.8618,  2.5182,  1.6739,\n",
      "        -2.2831,  1.1891])\n",
      "tensor([  8.2327,   3.3867, -13.0232,   8.7149,   5.0187,  -6.6933,  -5.5305,\n",
      "         10.1438,  -4.4911,   6.5221])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(model.layer2.weight.grad[0][0:10])\n",
    "\n",
    "for i in range(0, 5):\n",
    "    prediction = model(some_input)\n",
    "    loss = (ideal_output - prediction).pow(2).sum()\n",
    "    loss.backward()\n",
    "    \n",
    "print(model.layer2.weight.grad[0][0:10])\n",
    "\n",
    "optimizer.zero_grad(set_to_none=False)\n",
    "\n",
    "print(model.layer2.weight.grad[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e319b57-c571-4ec9-8a5c-6be1bfc27f1b",
   "metadata": {},
   "source": [
    "在运行上述单元后，你应该看到在多次运行 `loss.backward()` 后，大多数梯度的量级会大很多。在运行下一个训练批次之前，如果没有将梯度归零，将导致梯度以这种方式爆炸，造成不正确和不可预测的学习结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9343c675-96cd-44d0-ab78-1c6814534089",
   "metadata": {},
   "source": [
    "## Turning Autograd Off and On\n",
    "\n",
    "在有些情况下，你需要对是否启用autograd进行细粒度的控制。根据不同的情况，有多种方法可以做到这一点。\n",
    "\n",
    "最简单的是直接改变张量上的 `requires_grad` 标志："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3382fe93-0b8b-4267-ad73-e6a2cb4cbb4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]], grad_fn=<MulBackward0>)\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3, requires_grad=True)\n",
    "print(a)\n",
    "\n",
    "b1 = 2 * a\n",
    "print(b1)\n",
    "\n",
    "a.requires_grad = False\n",
    "b2 = 2 * a\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59b2ee-ef02-4bc6-ab27-f5555b5ea9ae",
   "metadata": {},
   "source": [
    "在上面的单元格中，我们看到 `b1` 有一个 `grad_fn` （即追踪的计算历史），这是我们所期望的，因为它是从一个打开了autograd的张量 `a` 派生的。当我们用 `a.requires_grad = False` 明确地关闭autograd时，计算历史就不再被追踪了，正如我们在计算 `b2` 后看到的。\n",
    "\n",
    "如果你只需要暂时关闭autograd，一个更好的方法是使用 `torch.no_grad()`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9abb1033-ce06-4382-a7a5-0a69169364a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n",
      "tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3, requires_grad=True) * 2\n",
    "b = torch.ones(2, 3, requires_grad=True) * 3\n",
    "\n",
    "c1 = a + b\n",
    "print(c1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    c2 = a + b\n",
    "\n",
    "print(c2)\n",
    "\n",
    "c3 = a * b\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabe864f-bc68-4fc2-86c5-6075a8170551",
   "metadata": {},
   "source": [
    "`torch.no_grad()` 也可以作为一个函数或方法修饰器使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f948ec5f-9248-4317-9d88-375960c313b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "def add_tensors1(x, y):\n",
    "    return x + y\n",
    "\n",
    "@torch.no_grad()\n",
    "def add_tensors2(x, y):\n",
    "    return x + y\n",
    "\n",
    "\n",
    "a = torch.ones(2, 3, requires_grad=True) * 2\n",
    "b = torch.ones(2, 3, requires_grad=True) * 3\n",
    "\n",
    "c1 = add_tensors1(a, b)\n",
    "print(c1)\n",
    "\n",
    "c2 = add_tensors2(a, b)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573950d-65d1-4604-a5d6-23c27ae99e80",
   "metadata": {},
   "source": [
    "有一个相应的上下文管理器，`torch.enable_grad()`，当它没有打开的时候，用于打开autograd。它也可以作为一个修饰器使用。\n",
    "\n",
    "最后，你可能有一个需要梯度跟踪的张量，但你想要一个不执行梯度跟踪的副本。为此，我们有张量对象的 `detach()` 方法，它会创建一个从计算历史中分离出来的张量副本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50c4d027-182b-43ac-a047-d4b4697eb2de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5365, 0.5231, 0.7058, 0.5468, 0.9089], requires_grad=True)\n",
      "tensor([0.5365, 0.5231, 0.7058, 0.5468, 0.9089])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, requires_grad=True)\n",
    "y = x.detach()\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9208302c-faf9-4add-9319-960c0e182d5f",
   "metadata": {},
   "source": [
    "正如我们上面所做的，当我们想对一些张量进行绘图时。因为 `matplotlib` 希望一个NumPy数组作为输入，而对于 `requires_grad=True` 的张量来说，从PyTorch张量到NumPy数组的隐式转换并没有启用。使用一个分离的副本可以让我们的方法得以正确执行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a5bba7-49f2-4ec7-9fc9-227b20d95955",
   "metadata": {},
   "source": [
    "## Autograd and In-place Operations\n",
    "\n",
    "到目前为止，在这个笔记本的每个例子中，我们都使用了变量来捕捉计算的中间值。Autograd需要这些中间值来进行梯度计算。出于这个原因，在使用autograd时，你必须谨慎使用原地操作。这样做会破坏你在 `backward()` 调用中计算导数所需的信息。如果你试图对需要autograd的叶子变量进行原地操作，PyTorch甚至会阻止你，如下所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede35fe-0506-453d-a103-cdbf59df71b1",
   "metadata": {},
   "source": [
    "> **小贴士**\n",
    ">\n",
    "> 下面的代码会抛出一个runtime error.这是意料之中的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3b2bb01-8a0a-49d2-a328-1234ef654f10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-cffbbcd7d649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "torch.sin_(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40fc9d4-54cd-40aa-89ba-9f9213e54a01",
   "metadata": {},
   "source": [
    "## Autograd Profiler\n",
    "\n",
    "Autograd详细地跟踪你的每一步计算。这样的计算历史，结合时间信息，将成为一个方便的剖析器--而autograd已经将这个功能内置于其中。下面是一个快速使用的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d07c605-d737-4e87-b0f3-18804688e1b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "    cudaGetDeviceProperties        64.96%       9.409ms        64.96%       9.409ms     940.900us       0.000us         0.00%       0.000us       0.000us            10  \n",
      "                  aten::div        17.56%       2.543ms        17.56%       2.543ms       2.543us       6.881ms        41.53%       6.881ms       6.881us          1000  \n",
      "                  aten::mul        17.23%       2.496ms        17.23%       2.496ms       2.496us       9.689ms        58.47%       9.689ms       9.689us          1000  \n",
      "      cudaDeviceSynchronize         0.23%      34.000us         0.23%      34.000us      34.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "      cudaStreamIsCapturing         0.01%       2.000us         0.01%       2.000us       0.067us       0.000us         0.00%       0.000us       0.000us            30  \n",
      "         cudaGetDeviceCount         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 14.484ms\n",
      "Self CUDA time total: 16.570ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "run_on_gpu = False\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    run_on_gpu = True\n",
    "\n",
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "y = torch.rand(2, 3, requires_grad=True)\n",
    "z = torch.ones(2, 3, requires_grad=True)\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n",
    "    for _ in range(1000):\n",
    "        z = (z / x) * y\n",
    "\n",
    "print(prf.key_averages().table(sort_by='self_cpu_time_total'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8847bec1-5484-4277-9caf-4b8087c6b00f",
   "metadata": {},
   "source": [
    "剖析器还可以标记代码的单个子块，按输入张量形状划分数据，并将数据导出为Chrome追踪工具文件。有关API的全部细节，请参见[文档](https://pytorch.org/docs/stable/autograd.html#profiler)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6396c9-8847-423f-bc9c-619afb068b6f",
   "metadata": {},
   "source": [
    "## Advanced Topic: More Autograd Detail and the High-Level API\n",
    "\n",
    "如果你有一个具有n维输入和m维输出的函数，$\\vec y = f(\\vec x)$，完整的梯度是每一个输出相对于每一个输入的导数的矩阵，称为*雅各布（Jacbian）矩阵*：\n",
    "\n",
    "$J=\\left(\\begin{array}{ccc}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x 1} & \\cdots & \\frac{\\partial y_m}{\\partial x n}\n",
    "\\end{array}\\right)$\n",
    "\n",
    "如果你有第二个函数，$l = g(\\vec y)$，它接受m维的输入（即与上述输出相同的维度），并返回一个标量的输出，你可以将其相对于 $\\vec y$ 的梯度表达为一个列向量，$v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y 1} & \\cdots & \\frac{\\partial l}{\\partial y m}\\end{array}\\right)^T$ ，这实际上只是一个单列的雅各布式。\n",
    "\n",
    "更具体地说，把第一个函数想象成你的PyTorch模型（可能有许多输入和许多输出），第二个函数是一个损失函数（模型的输出是输入，损失值是标量输出）。\n",
    "\n",
    "如果我们将第一个函数的雅各布矩阵乘以第二个函数的梯度，并应用链式法则，我们得到：\n",
    "\n",
    "$\n",
    "J^T \\cdot v=\\left(\\begin{array}{ccc}\n",
    "\\frac{\\partial y 1}{\\partial x 1} & \\cdots & \\frac{\\partial y m}{\\partial x 1} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y 1}{\\partial x n} & \\cdots & \\frac{\\partial y m}{\\partial x n}\n",
    "\\end{array}\\right)\\left(\\begin{array}{c}\n",
    "\\frac{\\partial l}{\\partial y 1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial l}{\\partial y m}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "\\frac{\\partial l}{\\partial x 1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial l}{\\partial x_n}\n",
    "\\end{array}\\right)\n",
    "$\n",
    "\n",
    "注意：你也可以使用等价的操作$v^T\\cdot J$，并得到一个行向量。\n",
    "\n",
    "由此产生的列向量是第二个函数相对于第一个函数的输入的梯度，或者在我们的模型和损失函数的情况下，损失相对于模型输入的梯度。\n",
    "\n",
    "**torch.autograd 是计算这些乘积的引擎。** 这就是我们如何在后向传播中积累学习权重的梯度。\n",
    "\n",
    "由于这个原因，`backward()` 调用也可以接受一个可选的向量输入。这个向量代表了一组张量上的梯度，它被乘以前面的autograd追踪张量的雅各布矩阵。让我们用一个小的向量试一下具体的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cc49308-4fe6-4717-9cda-fa1a72425184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 536.0791, -592.8583, 1029.8279], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd0321c-5a96-4a70-a9ec-3667eba65a09",
   "metadata": {},
   "source": [
    "如果我们现在试图调用 `y.backward()`，我们会得到一个运行时错误和一条信息，即梯度只能为标量输出隐式计算。对于一个多维输出，autograd 希望我们提供这三个输出的梯度，以便它能乘以雅各布矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d7da638-6b39-467f-8ce9-29df0e4b432a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float) # 表示梯度\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c72b78b-001a-42d5-b75c-773e0256ed83",
   "metadata": {},
   "source": [
    "(请注意，输出梯度都与2的幂有关--这也是我们所期望的重复加倍操作）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f09fb-026f-4e50-a3a3-66929a173079",
   "metadata": {},
   "source": [
    "## The High-Level API\n",
    "\n",
    "autograd上有一个API，让你直接访问重要的微分矩阵和向量操作。特别是，它允许你计算特定输入下的特定函数的Jacobian和Hessian矩阵。(Hessian与Jacobian相似，但表达了所有的部分二阶导数)。它还提供了与这些矩阵进行矢量乘积的方法。\n",
    "\n",
    "让我们来看看一个简单函数的Jacobian，对2个单元素输入进行评估："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6118c6c1-f16b-4c28-9567-32cf36b51320",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.5905]), tensor([0.7649]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[3.6096]]), tensor([[3.]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exp_adder(x, y):\n",
    "    return 2 * x.exp() + 3 * y\n",
    "\n",
    "inputs = (torch.rand(1), torch.rand(1)) # 函数的参数\n",
    "print(inputs)\n",
    "torch.autograd.functional.jacobian(exp_adder, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70abef5-4db9-461f-a592-14f2add91a4c",
   "metadata": {},
   "source": [
    "如果你仔细看，第一个输出应该等于$2e^x$（因为$e^x$的导数是$e^x$），第二个值应该为。\n",
    "\n",
    "当然，你也可以用高阶张量来做这个："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3da6af3-bf30-42a8-80e6-8d8e60cbdca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.1089, 0.5990, 0.4893]), tensor([0.5958, 0.6126, 0.7583]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[2.2300, 0.0000, 0.0000],\n",
       "         [0.0000, 3.6405, 0.0000],\n",
       "         [0.0000, 0.0000, 3.2624]]),\n",
       " tensor([[3., 0., 0.],\n",
       "         [0., 3., 0.],\n",
       "         [0., 0., 3.]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = (torch.rand(3), torch.rand(3)) # 函数的参数\n",
    "print(inputs)\n",
    "torch.autograd.functional.jacobian(exp_adder, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c012e4-720c-4f3a-a262-a54a2748461a",
   "metadata": {},
   "source": [
    "`torch.autograd.functional.hessian()` 方法的工作原理与此相同（假设你的函数是可两二阶导的），但会返回一个所有二介导数的矩阵。\n",
    "\n",
    "如果你提供了矢量，还有一个函数可以直接计算矢量-雅各比乘积："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "516ba25a-63a3-4988-bde8-04d6b5382a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-789.2786,  830.6387, -549.0214]),\n",
       " tensor([5.1200e+01, 5.1200e+02, 5.1200e-02]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_some_doubling(x):\n",
    "    y = x * 2\n",
    "    while y.data.norm() < 1000:\n",
    "        y = y * 2\n",
    "    return y\n",
    "\n",
    "inputs = torch.randn(3)\n",
    "my_gradients = torch.tensor([0.1, 1.0, 0.0001])\n",
    "torch.autograd.functional.vjp(do_some_doubling, inputs, v=my_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba702e7-a2fb-4909-822c-a87c930a9c29",
   "metadata": {},
   "source": [
    "`torch.autograd.functional.jvp()` 方法执行与 `vjp()` 相同的矩阵乘法，操作数相反。`vhp()` 和 `hvp()` 方法对vector-Hessian积进行同样的操作。\n",
    "\n",
    "欲了解更多信息，包括性能，请见[API文档](https://pytorch.org/docs/stable/autograd.html#functional-higher-level-api)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea932412-7362-48e0-8cf4-69cd03967d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpth",
   "language": "python",
   "name": "jpth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
