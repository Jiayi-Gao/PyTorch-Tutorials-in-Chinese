{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd12ebba-dd9e-44a9-8eab-bdaf0468ebcc",
   "metadata": {},
   "source": [
    "# TENSORS\n",
    "\n",
    "张量是一种特定的数据结构，与数组和矩阵非常相似。在PyTorch中，我们使用张量来编码一个模型的输入和输出，以及模型的参数。\n",
    "\n",
    "张量与[NumPy](https://numpy.org/)的ndarrays类似，只是张量可以在GPU或其他硬件加速器上运行。事实上，张量和NumPy数组通常可以共享相同的底层内存，不需要复制数据（见[与NumPy的桥接](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label)）。张量还为自动求导进行了优化（我们将在后面的[Autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)部分看到更多关于这一点）。如果你熟悉ndarrays，你就会对张量API的很熟悉。如果不熟悉，请跟上!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e1dec9-484d-4641-8dda-c47f5e042006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a5aa7a-3bf0-4197-8306-d76266a5b88d",
   "metadata": {},
   "source": [
    "## Initializing a Tensor\n",
    "\n",
    "张量可以用不同的方法进行初始化。请看下面的例子：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d77eb-c55c-404f-8d12-63a10317672e",
   "metadata": {},
   "source": [
    "### 直接从数据中创建\n",
    "\n",
    "张量可以直接使用数据创建。数据类型会自动匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0714d333-c48c-48bd-a0be-0dc0469db4db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e6155-7025-456f-a0f8-ebd0d63a1497",
   "metadata": {},
   "source": [
    "### 从NumPy数组中创建\n",
    "\n",
    "张量可以用NumPy数组创建（反之亦然，详见与[NumPy桥接](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0fdaa9-4842-4679-9e05-ac08737c8e08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad88c87-1255-481e-bfb6-6bcc3923293f",
   "metadata": {},
   "source": [
    "### 用其他张量创建\n",
    "\n",
    "除非明确的重写，新的张量会保留参数张量的属性（形状、数据类型）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50cab1de-3c87-41d6-ad54-664f5cf3a588",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.6656, 0.2030],\n",
      "        [0.3809, 0.8766]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # 创建了一个全1的张量，新张量保留了x_data的属性\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # 创建了一个随机数张量，重写了数据类型属性\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e48a0-8808-4223-8d05-9ee52d6b1475",
   "metadata": {},
   "source": [
    "### 用随机值或常数值创建\n",
    "\n",
    "`shape`是一个描述张量维度的元组。在下面的函数中，它定义了输出张量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "210aba69-223d-4a50-8bde-8150ff6dbb2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.5680, 0.4190, 0.5879],\n",
      "        [0.9622, 0.8923, 0.3943]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape) # 创建一个全0张量\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb9952-d887-4ecc-8965-4708ea71e52a",
   "metadata": {},
   "source": [
    "## 张量的属性\n",
    "\n",
    "张量的属性描述了他们的形状、数据类型和它们被存储在哪个设备上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17eca1f8-5654-48ac-a400-c51909a7724f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28b06a-e699-4129-b629-7b0c7c2a3ad0",
   "metadata": {},
   "source": [
    "## 张量上的操作\n",
    "\n",
    "[这里](https://pytorch.org/docs/stable/torch.html)全面介绍了100多个张量操作，包括算术、线性代数、矩阵操作（转置、索引、切片）、采样等。\n",
    "\n",
    "这些操作中的每一个都可以在GPU上运行（速度通常比在CPU上高）。如果你使用Colab，可以进入Runtime > Change runtime type > GPU来分配一个GPU。\n",
    "\n",
    "默认情况下，张量是在CPU上创建的。我们需要使用 `.to` 方法显式地将张量移动到GPU上（在检查GPU的可用性之后）。请注意，在不同的设备上复制大型的张量，在时间和内存上都是很昂贵的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18ed25de-990c-4708-85ad-6df3e1cf5f07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 如果GPU可用，我们就将张量移动到GPU上。\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627b6dbf-9bbf-467d-82d8-29eeec996714",
   "metadata": {},
   "source": [
    "尝试一下列表（list）中的一些操作。如果你熟悉NumPy API，你会发现Tensor API使用起来很容易。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89a073-ae3c-4daf-9eba-54a4786a4117",
   "metadata": {},
   "source": [
    "### 类似NumPy的索引和切片："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a8cfc38-5163-4292-b939-14976b8b267d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First colum: tensor([1., 1., 1., 1.])\n",
      "Last colum: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First colum: {tensor[:,0]}\")\n",
    "print(f\"Last colum: {tensor[...,-1]}\")\n",
    "tensor[:,1] = 0 # 将张量的第二列置为0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b4a206-1a5b-423b-82a1-0eb95709ddd3",
   "metadata": {},
   "source": [
    "**拼接张量** 你可以使用`torch.cat` 来沿着给定的维度拼接一组张量。也请参见[torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)，另一个与`torch.cat`有细微差别的张量连接运算符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b33bcef-c929-4c8c-bbd7-f9a95fef8ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim = 1) # dim = 1 表示按列拼接\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5971ad8-cf1d-45a0-b959-0a3273c720dd",
   "metadata": {},
   "source": [
    "### 算术运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ada5f63-77a7-4b55-81d4-c790b2136726",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 两个张量的矩阵乘法。y1, y2, y3具有相同的值。\n",
    "# ``tensor.T`` 返回一个张量的转置\n",
    "y1 = tensor @ tensor.T  # @是一个矩阵乘法运算符\n",
    "y2 = tensor.matmul(tensor.T) # .matmul 等价于 @\n",
    "\n",
    "y3 = torch.rand_like(y1) # 创建一个形状大小与 y1 相同的随机值张量\n",
    "torch.matmul(tensor, tensor.T, out=y3) # 矩阵相乘结果放入 y3 中\n",
    "\n",
    "\n",
    "# 张量之间对应元素相乘。z1, z2, z3具有相同的值。\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c88f7-527f-487e-975f-bca092682b29",
   "metadata": {},
   "source": [
    "**单元素张量**  如果你有一个单元素张量，例如通过将一个张量的所有值聚集成一个值，你可以使用 `item()` 将其转换为一个 Python 数值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdb240d6-098f-4d3a-bc46-d319031a498e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()  # 将张量tensor中所有的值求和\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ef114-d7c8-46c5-ba3d-1b4638374456",
   "metadata": {},
   "source": [
    "**原地操作** 将结果存储到操作数中的操作被称为原地操作。它们用后缀 `_` 来表示。例如：`x.copy_(y)`, `x.t_()`, 这样的操作将改变 `x` 本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "787b7c61-2f3b-49c8-8f6c-5291d35f0737",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a45d54-241f-4837-ad4d-54c1e95ea8b7",
   "metadata": {},
   "source": [
    "> **小贴士**\n",
    ">\n",
    "> 原地操作可以节省一些内存，但在计算导数时可能会出现问题，因为会丢失历史记录。因此，我们不鼓励使用这种操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f631ed40-e894-4c19-8ca9-731f84536498",
   "metadata": {},
   "source": [
    "## 与NumPy桥接\n",
    "\n",
    "CPU上的张量和NumPy数组可以共享它们的底层内存位置，所以改变一个将改变另一个。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae24686-360e-4f13-8f53-973da4aeb6a6",
   "metadata": {},
   "source": [
    "### 张量转NumPy数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52325f75-9033-47ab-b78f-7c82b0a94abc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea782a-7e00-4b58-8bc2-81b1794e841e",
   "metadata": {},
   "source": [
    "对tensor进行修改，将会反映到NumPy数组上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bb8dbfa-beba-4f3c-853b-c5492b077938",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812aec1-07d1-4d5f-a116-1570564f0689",
   "metadata": {},
   "source": [
    "### NumPy数组转张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ced1274-1165-40c8-a491-bc9d4a17c1f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1eb764-9472-4364-bd4e-2b5f5cc95a21",
   "metadata": {},
   "source": [
    "改变NumPy数组会反映到张量上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6e2fa26-a93a-46f9-bc5a-564de46d8ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00e0b7-8a1e-4cd5-b180-df1e99815868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpth",
   "language": "python",
   "name": "jpth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
