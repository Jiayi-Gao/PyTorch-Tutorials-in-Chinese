{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d140c728-4e3b-4af8-afef-8f769a73611c",
   "metadata": {},
   "source": [
    "# AUTOMATIC DIFFERENTIATION WITH `TORCH.AUTOGRAD`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149ddd9-5d48-47f7-9add-992095958803",
   "metadata": {},
   "source": [
    "在训练神经网络时，最常使用的算法是**反向传播**。在这种算法中，参数（模型权重）根据损失函数相对于给定参数的**梯度**来调整。\n",
    "\n",
    "为了计算这些梯度，PyTorch有一个内置的求导引擎称为`torch.autograd`。它支持对任何计算图自动计算梯度。\n",
    "\n",
    "以最简单的一层神经网络为例，输入为`X`，参数为`W`和`b`，和一些损失函数。它可以在PyTorch中以如下方式定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba92773-6986-4dd2-a1fe-5b8312edf371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5) # 输入张量\n",
    "y = torch.zeros(3) # 期望输出\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a25b4-a3c3-49c9-869d-3647f1be05aa",
   "metadata": {},
   "source": [
    "## Tensors, Functions and Computational graph\n",
    "\n",
    "这段代码定义了如下**计算图**：\n",
    "![](https://pytorch.org/tutorials/_images/comp-graph.png)\n",
    "在这个网络中，`w`和`b`是我们需要优化的参数。因此，我们需要能够计算损失函数相对于这些变量的梯度。为了做到这一点，我们设置了这些张量的 `requires_grad` 属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c773553-423d-4602-a82b-27f1a1df51cf",
   "metadata": {},
   "source": [
    "> **小贴士**\n",
    ">\n",
    ">你可以在创建张量时设置`requires_grad`的值，或者在创建张量之后使用`x.requires_grad_(True)`方法设置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a83bc-f2e5-4c71-a039-07c7ece32fa6",
   "metadata": {},
   "source": [
    "我们应用于张量来构建计算图的函数实际上是一个`Function`类的对象。这个对象知道如何在*前向*上计算函数，也知道如何在*后向传播*步骤中计算其导数。后向传播函数的引用被存储在张量的`grad_fn`属性中。你可以在[文档](https://pytorch.org/docs/stable/autograd.html#function)中找到更多关于`Function`的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ada88b-0494-4834-8ec9-774579cc098a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7f2180037ad0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f2180037a10>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb63131-3093-4164-b6e4-002648f749f5",
   "metadata": {},
   "source": [
    "## Computing Gradients\n",
    "\n",
    "为了优化神经网络中的参数权重，我们需要计算损失函数相对于参数的导数,也就是我们需要在一些固定的`x`和`y`值下计算$ \\frac{\\partial loss}{\\partial w} $和$ \\frac{\\partial loss}{\\partial b} $。为了计算这些导数，我们调用`loss.backward()`，然后从`w.grad`和`b.grad`中检索数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d20d142-d33c-43ba-b9df-ad129e03bc08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0050, 0.2721, 0.1239],\n",
      "        [0.0050, 0.2721, 0.1239],\n",
      "        [0.0050, 0.2721, 0.1239],\n",
      "        [0.0050, 0.2721, 0.1239],\n",
      "        [0.0050, 0.2721, 0.1239]])\n",
      "tensor([0.0050, 0.2721, 0.1239])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263ce07c-626d-4f87-9bc0-9c0c0be2ac23",
   "metadata": {},
   "source": [
    "> **小贴士**\n",
    ">\n",
    "> 我们只能从将`requires_grad`属性设置为`True`的计算图中的叶子节点获取`grad`属性。对于其他所有图中节点，梯度不可用。\n",
    ">\n",
    "> 出于性能方面的考虑，我们在一个给定的计算图上只能使用`backward`进行一次梯度计算。如果我们需要在同一个图形上进行多次`backward`调用，我们需要在`backward`调用中传递 `retain_graph=True`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da1333-b865-4d1d-aedf-8ccbd8d67628",
   "metadata": {},
   "source": [
    "## Disabling Gradient Tracking\n",
    "\n",
    "在默认情况下，所有设置`requires_grad=True`的张量的计算过程会被追踪并且支持梯度计算。然而在一些情况下我们并不需要这么做，例如当我们已经训练好了一个模型并且想把它应用于输入数据，也就是说我们只想在网络中做*前向*计算。我们可以使用`torch.no_grad()`块包裹我们的计算代码来停止追踪计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86b21729-fa73-4459-88a7-768736e74fc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127f0cb2-e986-49e5-8b0f-af9b3fc87257",
   "metadata": {},
   "source": [
    "另一种达到同样结果的方式是对张量使用`detach()`方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f8292e9-81dd-485c-849b-a84c1523d7e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf37ba9-d6bb-4cb2-9172-1856ad71e263",
   "metadata": {},
   "source": [
    "这里是一些你想要禁用梯度追踪可能的原因：\n",
    "- 将你的神经网络中的一些参数标记为**冻结参数**。\n",
    "- 当只进行前向传播时进行**加速计算**，因为对张量进行计算时禁用梯度追踪会更高效。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b1cd6a-eb87-4306-8c97-a5db1b62e9da",
   "metadata": {},
   "source": [
    "## More on Computational Graphs\n",
    "\n",
    "从概念上讲，autograd在一个由[Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)对象组成的有向无环图（DAG）中保存了数据（张量）和所有执行的操作（以及产生的新张量）的记录。在这个DAG中，叶子是输入张量，根是输出张量。通过追踪这个图从根到叶，你可以使用链式规则自动计算梯度。\n",
    "\n",
    "在前向传播中，autograd同时做了两件事：\n",
    "- 运行给定的运算来计算一个结果向量。\n",
    "- 在DAG中保存运算的梯度函数。\n",
    "\n",
    "当在DAG的根调用`.backward()`时，后向传递就会启动，然后`.autograd`：\n",
    "- 从每个`.grad_fn`计算梯度，\n",
    "- 在每个张量的`.grad`属性积累它们\n",
    "- 使用链式法则，在每条通往叶子张量的路径上传播。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aee36c-4350-46f8-a53c-7b9807689fa9",
   "metadata": {},
   "source": [
    "> **小贴士**\n",
    ">\n",
    "> **DAGs在PyTorch中是动态的** 需要注意的是，计算图是从头开始重新创建的；在每次调用`.backward()`后，autograd开始填充一个新的图形。这正是允许你在模型中使用控制流语句的原因；如果需要，你可以在每次迭代时改变形状、大小和操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e708c2-303e-41f0-8968-9dd0346211ab",
   "metadata": {},
   "source": [
    "## 选读：张量梯度和雅各比（Jacobian）乘积\n",
    "\n",
    "在许多情况下，我们有一个标量损失函数，我们需要计算相对于某些参数的梯度。然而，有些情况下，输出函数是一个任意的张量。在这种情况下，PyTorch允许你计算所谓的**雅各比乘积**，而不是实际的梯度。\n",
    "\n",
    "对于一个向量函数$\\vec y = f(\\vec x)$， 当$\\vec x = \\langle x_1, \\cdots, x_n \\rangle$ 且 $\\vec y = \\langle y_1, \\cdots, y_n \\rangle$时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62350493-d92f-49a8-80e1-cbb1d8b16620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpth",
   "language": "python",
   "name": "jpth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
